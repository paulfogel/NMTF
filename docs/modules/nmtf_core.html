<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>nmtf.modules.nmtf_core API documentation</title>
<meta name="description" content="Non-negative matrix and tensor factorization core functions" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nmtf.modules.nmtf_core</code></h1>
</header>
<section id="section-intro">
<p>Non-negative matrix and tensor factorization core functions</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Non-negative matrix and tensor factorization core functions

&#34;&#34;&#34;

# Author: Paul Fogel

# License: MIT
# Jan 4, &#39;20

import math
import numpy as np
#from sklearn.utils.extmath import randomized_svd
from tqdm import tqdm
from scipy.stats import hypergeom
from scipy.optimize import nnls

from .nmtf_utils import *

def NMFProjGrad(V, Vmis, W, Hinit, NMFAlgo, lambdax, tol, MaxIterations, NMFPriors):
    &#34;&#34;&#34;Projected gradient
    Code and notations adapted from Matlab code, Chih-Jen Lin
    Input:
        V: Input matrix
        Vmis: Define missing values (0 = missing cell, 1 = real cell)
        W: Left factoring vectors (fixed)
        Hinit: Right factoring vectors (initial values)
        NMFAlgo: =1,3: Divergence; =2,4: Least squares;
        lambdax: Sparseness parameter
            =-1: no penalty
            &lt; 0: Target percent zeroed rows in H
            &gt; 0: Current penalty
        tol: Tolerance
        MaxIterations: max number of iterations to achieve norm(projected gradient) &lt; tol
        NMFPriors: Elements in H that should be updated (others remain 0)
    Output:
        H: Estimated right factoring vectors
        tol: Current level of the tolerance
        lambdax: Current level of the penalty
    
    Reference
    ---------

    C.J. Lin (2007) Projected Gradient Methods for Non-negative Matrix Factorization
    Neural Comput. 2007 Oct;19(10):2756-79.

    &#34;&#34;&#34;
    H = Hinit
    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    n_Vmis = Vmis.shape[0]
    n, p = np.shape(V)
    n, nc = np.shape(W)
    alpha = 1

    if (NMFAlgo == 2) or (NMFAlgo == 4):
        beta = .1
        if n_Vmis &gt; 0:
            WtV = W.T @ (V * Vmis)
        else:
            WtV = W.T @ V
            WtW = W.T @ W
    else:
        beta = .1
        if n_Vmis &gt; 0:
            WtWH = W.T @ Vmis
        else:
            WtWH = W.T @ np.ones((n, p))

    if (lambdax &lt; 0) &amp; (lambdax != -1):
        H0 = H

    restart = True
    while restart:
        for iIter in range(1, MaxIterations + 1):
            addPenalty = 0
            if lambdax != -1:
                addPenalty = 1

            if (NMFAlgo == 2) or (NMFAlgo == 4):
                if n_Vmis &gt; 0:
                    WtWH = W.T @ ((W @ H) * Vmis)
                else:
                    WtWH = WtW @ H
            else:
                if n_Vmis &gt; 0:
                    WtV = W.T @ ((V * Vmis) / (W @ H))
                else:
                    WtV = W.T @ (V / (W @ H))

            if lambdax &gt; 0:
                grad = WtWH - WtV + lambdax
            else:
                grad = WtWH - WtV

            projgrad = np.linalg.norm(grad[(grad &lt; 0) | (H &gt; 0)])

            if projgrad &gt;= tol:
                # search step size
                for inner_iter in range(1, 21):
                    Hn = H - alpha * grad
                    Hn[np.where(Hn &lt; 0)] = 0
                    if n_NMFPriors &gt; 0:
                        Hn = Hn * NMFPriors

                    d = Hn - H
                    gradd = np.sum(grad * d)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Vmis &gt; 0:
                            dQd = np.sum((W.T @ ((W @ d) * Vmis)) * d)
                        else:
                            dQd = np.sum((WtW @ d) * d)
                    else:
                        if n_Vmis &gt; 0:
                            dQd = np.sum((W.T @ ((W @ d) * (Vmis / (W @ H)))) * d)
                        else:
                            dQd = np.sum((W.T @ ((W @ d) / (W @ H))) * d)

                    suff_decr = (0.99 * gradd + 0.5 * dQd &lt; 0)
                    if inner_iter == 1:
                        decr_alpha = not suff_decr
                        Hp = H

                    if decr_alpha:
                        if suff_decr:
                            H = Hn
                            break
                        else:
                            alpha = alpha * beta
                    else:
                        if (suff_decr == False) | (np.where(Hp != Hn)[0].size == 0):
                            H = Hp
                            break
                        else:
                            alpha = alpha / beta
                            Hp = Hn
                # End for (inner_iter

                if (lambdax &lt; 0) &amp; addPenalty:
                    # Initialize penalty
                    lambdax = percentile_exc(H[np.where(H &gt; 0)], -lambdax * 100)
                    H = H0
                    alpha = 1
            else:  # projgrad &lt; tol
                if (iIter == 1) &amp; (projgrad &gt; 0):
                    tol /= 10
                else:
                    restart = False

                break
            #       End if projgrad

            if iIter == MaxIterations:
                restart = False
        #   End For iIter

    H = H.T
    return [H, tol, lambdax]

def NMFProjGradKernel(Kernel, V, Vmis, W, Hinit, NMFAlgo, tol, MaxIterations, NMFPriors):
    &#34;&#34;&#34;Projected gradient, kernel version
    Code and notations adapted from Matlab code, Chih-Jen Lin
    Input:
        Kernel: Kernel used
        V: Input matrix
        Vmis: Define missing values (0 = missing cell, 1 = real cell)
        W: Left factoring vectors (fixed)
        Hinit: Right factoring vectors (initial values)
        NMFAlgo: =1,3: Divergence; =2,4: Least squares;
        tol: Tolerance
        MaxIterations: max number of iterations to achieve norm(projected gradient) &lt; tol
        NMFPriors: Elements in H that should be updated (others remain 0)
    Output:
        H: Estimated right factoring vectors
        tol: Current level of the tolerance
    
    Reference
    ---------

    C.J. Lin (2007) Projected Gradient Methods for Non-negative Matrix Factorization
        Neural Comput. 2007 Oct;19(10):2756-79.

    &#34;&#34;&#34;
    H = Hinit.T
    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    n_Vmis = Vmis.shape[0]
    n, p = np.shape(V)
    p, nc = np.shape(W)
    alpha = 1
    VW = V @ W

    if (NMFAlgo == 2) or (NMFAlgo == 4):
        beta = .1
        if n_Vmis &gt; 0:
            WtV = VW.T @ (V * Vmis)
        else:
            WtV = W.T @ Kernel
            WtW = W.T @ Kernel @ W
    else:
        beta = .1
        MaxIterations = round(MaxIterations/10)
        if n_Vmis &gt; 0:
            WtWH = VW.T @ Vmis
        else:
            WtWH = VW.T @ np.ones((n, p))

    restart = True
    while restart:
        for iIter in range(1, MaxIterations + 1):
            if (NMFAlgo == 2) or (NMFAlgo == 4):
                if n_Vmis &gt; 0:
                    WtWH = VW.T @ ((VW @ H) * Vmis)
                else:
                    WtWH = WtW @ H
            else:
                if n_Vmis &gt; 0:
                    WtV = VW.T @ ((V * Vmis) / (VW @ H))
                else:
                    WtV = VW.T @ (V / (VW @ H))

            grad = WtWH - WtV
            projgrad = np.linalg.norm(grad[(grad &lt; 0) | (H &gt; 0)])
            if projgrad &gt;= tol:
                # search step size
                for inner_iter in range(1, 21):
                    Hn = H - alpha * grad
                    Hn[np.where(Hn &lt; 0)] = 0
                    if n_NMFPriors &gt; 0:
                        Hn = Hn * NMFPriors

                    d = Hn - H
                    gradd = np.sum(grad * d)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Vmis &gt; 0:
                            dQd = np.sum((VW.T @ ((VW @ d) * Vmis)) * d)
                        else:
                            dQd = np.sum((WtW @ d) * d)
                    else:
                        if n_Vmis &gt; 0:
                            dQd = np.sum((VW.T @ ((VW @ d) * (Vmis / (VW @ H)))) * d)
                        else:
                            dQd = np.sum((VW.T @ ((VW @ d) / (VW @ H))) * d)

                    suff_decr = (0.99 * gradd + 0.5 * dQd &lt; 0)
                    if inner_iter == 1:
                        decr_alpha = not suff_decr
                        Hp = H

                    if decr_alpha:
                        if suff_decr:
                            H = Hn
                            break
                        else:
                            alpha = alpha * beta
                    else:
                        if (suff_decr == False) | (np.where(Hp != Hn)[0].size == 0):
                            H = Hp
                            break
                        else:
                            alpha = alpha / beta
                            Hp = Hn
                # End for (inner_iter
            else:  # projgrad &lt; tol
                if iIter == 1:
                    tol /= 10
                else:
                    restart = False

                break
            #       End if projgrad

            if iIter == MaxIterations:
                restart = False
        #   End For iIter

    H = H.T
    return [H, tol]

def NMFApplyKernel(M, NMFKernel, Mt, Mw):
    &#34;&#34;&#34;Calculate kernel (used with convex NMF)
    Input:
        M: Input matrix
        NMFKernel: Type of kernel
            =-1: linear
            = 2: quadratic
            = 3: radiant
        Mt: Left factoring matrix
        Mw: Right factoring matrix
    Output:
        Kernel
    &#34;&#34;&#34;

    n, p = M.shape
    try:
        p, nc = Mw.shape
    except:
        nc = 0

    if NMFKernel == 1:
        Kernel = M.T @ M
    elif NMFKernel == 2:
        Kernel = (np.identity(p) + M.T @ M) ** 2
    elif NMFKernel == 3:
        Kernel = np.identity(p)
        # Estimate Sigma2
        Sigma2 = 0

        for k1 in range(1, nc):
            for k2 in range(0, k1):
                Sigma2 = max(Sigma2, np.linalg.norm(Mt[:, k1] - Mt[:, k2]) ** 2)

        Sigma2 /= nc
        for j1 in range(1, p):
            for j2 in range(0, j1):
                Kernel[j1, j2] = math.exp(-np.linalg.norm(M[:, j1] - M[:, j2]) ** 2 / Sigma2)
                Kernel[j2, j1] = Kernel[j1, j2]

    return Kernel

def NMFReweigh(M, Mt, NMFPriors, AddMessage):
    &#34;&#34;&#34;Overload skewed variables (used with deconvolution only)
    Input:
         M: Input matrix
         Mt: Left hand matrix
         NMFPriors: priors on right hand matrix
    Output:
         NMFPriors: updated priors

    Note: This code is still experimental

    &#34;&#34;&#34;
    ErrMessage = &#34;&#34;
    n, p = M.shape
    n_NMFPriors, nc = NMFPriors.shape
    NMFPriors[NMFPriors &gt; 0] = 1
    ID = np.where(np.sum(NMFPriors, axis=1) &gt; 1)
    n_ID = ID[0].shape[0]
    if n_ID == p:
        ErrMessage = &#39;Error! All priors are ambiguous.\nYou may uncheck the option in tab irMF+.&#39;
        return [NMFPriors, AddMessage, ErrMessage]

    NMFPriors[ID, :] = 0
    Mweight = np.zeros((p, nc))
    for k in range(0, nc):
        ID = np.where(NMFPriors[:, k] &gt; 0)
        pk = ID[0].shape[0]
        if pk == 0:
            ErrMessage = &#39;Error! Null column in NMF priors (&#39; + str(k+1) + &#39;, pre outlier filtering)&#39;
            return [NMFPriors, AddMessage, ErrMessage]

        Mc = np.zeros((n, p))

        # Exclude variables with outliers
        NInterQuart = 1.5
        for j in range(0, pk):
            Quart75 = percentile_exc(M[:, ID[0][j]], 75)
            Quart25 = percentile_exc(M[:, ID[0][j]], 25)
            InterQuart = Quart75 - Quart25
            MaxBound = Quart75 + NInterQuart * InterQuart
            MinBound = Quart25 - NInterQuart * InterQuart
            if np.where((M[:, ID[0][j]] &lt; MinBound) | (M[:, ID[0][j]] &gt; MaxBound))[0].shape[0] == 1:
                NMFPriors[ID[0][j], k] = 0

        ID = np.where(NMFPriors[:, k] &gt; 0)
        pk = ID[0].shape[0]
        if pk == 0:
            ErrMessage = &#39;Error! Null column in NMF priors (&#39; + str(k+1) + &#39;, post outlier filtering)&#39;
            return [NMFPriors, AddMessage, ErrMessage]

        # Characterize clusters by skewness direction
        Mtc = Mt[:, k] - np.mean(Mt[:, k])
        std = math.sqrt(np.mean(Mtc ** 2))
        skewness = np.mean((Mtc / std) ** 3) * math.sqrt(n * (n - 1)) / (n - 2)

        # Scale columns and initialized weights
        for j in range(0, pk):
            M[:, ID[0][j]] /= np.sum(M[:, ID[0][j]])
            Mc[:, ID[0][j]] = M[:, ID[0][j]] - np.mean(M[:, ID[0][j]])
            std = math.sqrt(np.mean(Mc[:, ID[0][j]] ** 2))
            Mweight[ID[0][j], k] = np.mean((Mc[:, ID[0][j]] / std) ** 3) * math.sqrt(n * (n - 1)) / (n - 2)

        if skewness &lt; 0:
            # Negative skewness =&gt; Component identifiable through small proportions
            Mweight[Mweight[:, k] &gt; 0, k] = 0
            Mweight = -Mweight
            IDneg = np.where(Mweight[:, k] &gt; 0)
            Nneg = IDneg[0].shape[0]
            if Nneg == 0:
                ErrMessage = &#39;Error! No marker variable found in component &#39; + str(k+1)
                return [NMFPriors, AddMessage, ErrMessage]

            AddMessage.insert(len(AddMessage),
                              &#39;Component &#39; + str(k+1) + &#39;: compositions are negatively skewed (&#39; + str(
                                  Nneg) + &#39; active variables)&#39;)
        else:
            # Positive skewness =&gt; Component identifiable through large proportions
            Mweight[Mweight[:, k] &lt; 0, k] = 0
            IDpos = np.where(Mweight[:, k] &gt; 0)
            Npos = IDpos[0].shape[0]
            if Npos == 0:
                ErrMessage = &#39;Error! No marker variable found in component &#39; + str(k+1)
                return [NMFPriors, AddMessage, ErrMessage]

            AddMessage.insert(len(AddMessage),
                              &#39;Component &#39; + str(k+1) + &#39;: compositions are positively skewed (&#39; + str(
                                  Npos) + &#39; active variables)&#39;)

        # Logistic transform of non-zero weights
        ID2 = np.where(Mweight[:, k] &gt; 0)
        n_ID2 = ID2[0].shape[0]
        if n_ID2 &gt; 1:
            mu = np.mean(Mweight[ID2[0], k])
            std = np.std(Mweight[ID2[0], k])
            Mweight[ID2[0], k] = (Mweight[ID2[0], k] - mu) / std
            Mweight[ID2[0], k] = np.ones(n_ID2) - np.ones(n_ID2) / (np.ones(n_ID2) + np.exp(
                2 * (Mweight[ID2[0], k] - percentile_exc(Mweight[ID2[0], k], 90))))
        else:
            Mweight[ID2[0], k] = 1

        # ReWeigh columns
        M[:, ID[0]] = M[:, ID[0]] * Mweight[ID[0], k].T

        # Update NMF priors (cancel columns with 0 weight &amp; replace non zero values by 1)
        NMFPriors[ID[0], k] = NMFPriors[ID[0], k] * Mweight[ID[0], k]
        ID = np.where(NMFPriors[:, k] &gt; 0)
        if ID[0].shape[0] &gt; 0:
            NMFPriors[ID[0], k] = 1
            # Scale parts
            M[:, ID[0]] /= np.linalg.norm(M[:, ID[0]])
        else:
            ErrMessage = &#39;Error! Null column in NMF priors (&#39; + str(k+1) + &#39;, post cancelling 0-weight columns)&#39;
            return [NMFPriors, AddMessage, ErrMessage]

    return [NMFPriors, AddMessage, ErrMessage]

def NMFSolve(M, Mmis, Mt0, Mw0, nc, tolerance, precision, LogIter, Status0, MaxIterations, NMFAlgo,
             NMFFixUserLHE, NMFFixUserRHE, NMFMaxInterm, NMFMaxIterProj, NMFSparseLevel,
             NMFFindParts, NMFFindCentroids, NMFKernel, NMFReweighColumns, NMFPriors, flagNonconvex, AddMessage,
             myStatusBox):
    &#34;&#34;&#34;
    Estimate left and right hand matrices
    Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         nc: NMF rank
         tolerance: Convergence threshold
         precision: Replace 0-value in multiplication rules
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFAlgo: =1,3: Divergence; =2,4: Least squares;
         NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
         NMFFixUserRHE: = 1 =&gt; fixed  right hand matrix columns
         NMFMaxInterm: Max iterations for warmup multiplication rules
         NMFMaxIterProj: Max iterations for projected gradient
         NMFSparseLevel: Requested sparsity in terms of relative number of rows with 0 values in right hand matrix
         NMFFindParts: Enforce convexity on left hand matrix
         NMFFindCentroids: Enforce convexity on right hand matrix
         NMFKernel: Type of kernel used; 1: linear; 2: quadratic; 3: radial
         NMFReweighColumns: Reweigh columns in 2nd step of parts-based NMF
         NMFPriors: Priors on right hand matrix
         flagNonconvex: Non-convexity flag on left hand matrix
    Output:
         Mt: Left hand matrix
         Mw: Right hand matrix
         diff: objective cost
         Mh: Convexity matrix
         NMFPriors: Updated priors on right hand matrix
         flagNonconvex: Updated non-convexity flag on left hand matrix
    
    Reference
    ---------

    C. H.Q. Ding et al (2010) Convex and Semi-Nonnegative Matrix Factorizations
    IEEE Transactions on Pattern Analysis and Machine Intelligence Vol: 32 Issue: 1

    &#34;&#34;&#34;
    ErrMessage = &#39;&#39;
    cancel_pressed = 0

    n, p = M.shape
    n_Mmis = Mmis.shape[0]
    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    nc = int(nc)
    nxp = int(n * p)
    Mh = np.array([])
    Mt = np.copy(Mt0)
    Mw = np.copy(Mw0)
    diff = 1.e+99

    # Add weights
    if n_NMFPriors &gt; 0:
        if NMFReweighColumns &gt; 0:
            # A local copy of M will be updated
            M = np.copy(M)
            NMFPriors, AddMessage, ErrMessage = NMFReweigh(M, Mt, NMFPriors, AddMessage)
            if ErrMessage != &#34;&#34;:
                return [Mt, Mw, diff, Mh, NMFPriors, flagNonconvex, AddMessage, ErrMessage, cancel_pressed]
        else:
            NMFPriors[NMFPriors &gt; 0] = 1

    if (NMFFindParts &gt; 0) &amp; (NMFFixUserLHE &gt; 0):
        NMFFindParts = 0

    if (NMFFindCentroids &gt; 0) &amp; (NMFFixUserRHE &gt; 0):
        NMFFindCentroids = 0
        NMFKernel = 1

    if (NMFFindCentroids &gt; 0) &amp; (NMFKernel &gt; 1):
        if n_Mmis &gt; 0:
            NMFKernel = 1
            AddMessage.insert(len(AddMessage), &#39;Warning: Non linear kernel canceled due to missing values.&#39;)

        if (NMFAlgo == 1) or (NMFAlgo == 3) :
            NMFKernel = 1
            AddMessage.insert(len(AddMessage), &#39;Warning: Non linear kernel canceled due to divergence minimization.&#39;)

    if n_NMFPriors &gt; 0:
        MwUser = NMFPriors
        for k in range(0, nc):
            if (NMFAlgo == 2) | (NMFAlgo == 4):
                Mw[:, k] = MwUser[:, k] / np.linalg.norm(MwUser[:, k])
            else:
                Mw[:, k] = MwUser[:, k] / np.sum(MwUser[:, k])

    MultOrPgrad = 1  # Start with Lee-Seung mult rules
    MaxIterations += NMFMaxInterm  # NMFMaxInterm Li-Seung iterations initialize projected gradient

    StepIter = math.ceil(MaxIterations / 10)
    pbar_step = 100 * StepIter / MaxIterations

    iIter = 0
    cont = 1

    # Initialize penalty
    # lambda = -1: no penalty
    # lambda = -abs(NMFSparselevel) : initialisation by NMFSparselevel (in negative value)
    if NMFSparseLevel &gt; 0:
        lambdaw = -NMFSparseLevel
        lambdat = -1
    elif NMFSparseLevel &lt; 0:
        lambdat = NMFSparseLevel
        lambdaw = -1
    else:
        lambdaw = -1
        lambdat = -1

    PercentZeros = 0
    iterSparse = 0
    NMFConvex = 0
    NLKernelApplied = 0

    myStatusBox.init_bar(delay=1)
    
    # Start loop
    while (cont == 1) &amp; (iIter &lt; MaxIterations):
        # Update RHE
        if NMFFixUserRHE == 0:
            if MultOrPgrad == 1:
                if (NMFAlgo == 2) or (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Mw = \
                            Mw * ((Mt.T @ (M * Mmis)) / (
                                    Mt.T @ ((Mt @ Mw.T) * Mmis) + precision)).T
                    else:
                        Mw = \
                             Mw * ((Mt.T @ M) / (
                                (Mt.T @ Mt) @ Mw.T + precision)).T
                else:
                    if n_Mmis &gt; 0:
                        Mw = Mw * (((M * Mmis) / ((Mt @ Mw.T) * Mmis + precision)).T @ Mt)
                    else:
                        Mw = Mw * ((M / (Mt @ Mw.T + precision)).T @ Mt)

                if n_NMFPriors &gt; 0:
                    Mw = Mw * NMFPriors
            else:
                # Projected gradient
                if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                    Mw, tolMw = NMFProjGradKernel(Kernel, M, Mmis, Mh, Mw, NMFAlgo, tolMw, NMFMaxIterProj, NMFPriors.T)
                elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0):
                    Mh, tolMh, dummy = NMFProjGrad(In, np.array([]), Mt, Mh.T, NMFAlgo, -1, tolMh, NMFMaxIterProj, np.array([]))
                else:
                    Mw, tolMw, lambdaw = NMFProjGrad(M, Mmis, Mt, Mw.T, NMFAlgo, lambdaw, tolMw, \
                                                                NMFMaxIterProj, NMFPriors.T)

            if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                for k in range(0, nc):
                    ScaleMw = np.linalg.norm(Mw[:, k])
                    Mw[:, k] = Mw[:, k] / ScaleMw
                    Mt[:, k] = Mt[:, k] * ScaleMw

        # Update LHE
        if NMFFixUserLHE == 0:
            if MultOrPgrad == 1:
                if (NMFAlgo == 2) | (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Mt = \
                            Mt * ((M * Mmis) @ Mw / (
                                ((Mt @ Mw.T) * Mmis) @ Mw + precision))
                    else:
                        Mt = \
                            Mt * (M @ Mw / (Mt @ (Mw.T @ Mw) + precision))
                else:
                    if n_Mmis &gt; 0:
                        Mt = Mt * (((M * Mmis).T / (Mw @ Mt.T + precision)).T @ Mw)
                    else:
                        Mt = Mt * ((M.T / (Mw @ Mt.T + precision)).T @ Mw)
            else:
                # Projected gradient
                if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                    Mh, tolMh, dummy = NMFProjGrad(Ip, np.array([]), Mw, Mh.T, NMFAlgo, -1, tolMh, NMFMaxIterProj, np.array([]))
                elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0):
                    Mt, tolMt = NMFProjGradKernel(Kernel, M.T, Mmis.T, Mh, Mt, NMFAlgo, tolMt, NMFMaxIterProj, np.array([]))
                else:
                    Mt, tolMt, lambdat = NMFProjGrad(M.T, Mmis.T, Mw, Mt.T, NMFAlgo,
                                                            lambdat, tolMt, NMFMaxIterProj, np.array([]))

            # Scaling
            if ((NMFConvex == 0) | (NMFFindCentroids &gt; 0)) &amp; (NMFFixUserLHE == 0) &amp;  (NMFFixUserRHE == 0):
                for k in range(0, nc):
                    if (NMFAlgo == 2) | (NMFAlgo == 4):
                        ScaleMt = np.linalg.norm(Mt[:, k])
                    else:
                        ScaleMt = np.sum(Mt[:, k])

                    if ScaleMt &gt; 0:
                        Mt[:, k] = Mt[:, k] / ScaleMt
                        if MultOrPgrad == 2:
                            Mw[:, k] = Mw[:, k] * ScaleMt

        # Switch to projected gradient
        if iIter == NMFMaxInterm:
            MultOrPgrad = 2
            StepIter = 1
            pbar_step = 100 / MaxIterations
            gradMt = Mt @ (Mw.T @ Mw) - M @ Mw
            gradMw = ((Mt.T @ Mt) @ Mw.T - Mt.T @ M).T
            initgrad = np.linalg.norm(np.concatenate((gradMt, gradMw), axis=0))
            tolMt = 1e-3 * initgrad
            tolMw = tolMt

        if iIter % StepIter == 0:
            if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                MhtKernel = Mh.T @ Kernel
                diff = (TraceKernel + np.trace(-2 * Mw @ MhtKernel + Mw @ (MhtKernel @ Mh) @ Mw.T)) / nxp
            elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0):
                MhtKernel = Mh.T @ Kernel
                diff = (TraceKernel + np.trace(-2 * Mt @ MhtKernel + Mt @ (MhtKernel @ Mh) @ Mt.T)) / nxp
            else:
                if (NMFAlgo == 2) | (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Mdiff = (Mt @ Mw.T - M) * Mmis
                    else:
                        Mdiff = Mt @ Mw.T - M

                else:
                    MF0 = Mt @ Mw.T
                    Mdiff = M * np.log(M / MF0 + precision) + MF0 - M

                diff = (np.linalg.norm(Mdiff)) ** 2 / nxp

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)

            if NMFSparseLevel != 0:
                if NMFSparseLevel &gt; 0:
                    lambdax = lambdaw
                else:
                    lambdax = lambdat

                Status = Status + &#39;; Achieved sparsity: &#39; + str(round(PercentZeros, 2)) + &#39;; Penalty: &#39; + str(
                    round(lambdax, 2))
                if LogIter == 1:
                    myStatusBox.myPrint(Status) 
            elif (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                Status = Status + &#39; - Find parts&#39;
            elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0) &amp; (NLKernelApplied == 0):
                Status = Status + &#39; - Find centroids&#39;
            elif NLKernelApplied == 1:
                Status = Status + &#39; - Apply non linear kernel&#39;

            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [Mt, Mw, diff, Mh, NMFPriors, flagNonconvex, AddMessage, ErrMessage, cancel_pressed]

            if LogIter == 1:
                if (NMFAlgo == 2) | (NMFAlgo == 4):
                    myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))
                else:
                    myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; DIV: &#34; + str(diff))

            if iIter &gt; NMFMaxInterm:
                if (diff0 - diff) / diff0 &lt; tolerance:
                    cont = 0

            diff0 = diff

        iIter += 1

        if (cont == 0) | (iIter == MaxIterations):
            if ((NMFFindParts &gt; 0) | (NMFFindCentroids &gt; 0)) &amp; (NMFConvex == 0):
                # Initialize convexity
                NMFConvex = 1
                diff0 = 1.e+99
                iIter = NMFMaxInterm + 1
                myStatusBox.init_bar(delay=1)
                cont = 1
                if NMFFindParts &gt; 0:
                    Ip = np.identity(p)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis * M, 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M, 1, np.array([]), np.array([]))
                    else:
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis * (M / (Mt @ Mw.T)), 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M / (Mt @ Mw.T), 1, np.array([]), np.array([]))

                    TraceKernel = np.trace(Kernel)
                    try:
                        Mh = Mw @ np.linalg.inv(Mw.T @ Mw)
                    except:
                        Mh = Mw @ np.linalg.pinv(Mw.T @ Mw)

                    Mh[np.where(Mh &lt; 0)] = 0
                    for k in range(0, nc):
                        ScaleMw = np.linalg.norm(Mw[:, k])
                        Mw[:, k] = Mw[:, k] / ScaleMw
                        Mh[:, k] = Mh[:, k] * ScaleMw

                    gradMh = Mh @ (Mw.T @ Mw) - Mw
                    gradMw = ((Mh.T @ Mh) @ Mw.T - Mh.T).T
                    initgrad = np.linalg.norm(np.concatenate((gradMh, gradMw), axis=0))
                    tolMh = 1.e-3 * initgrad
                    tolMw = tolMt
                elif NMFFindCentroids &gt; 0:
                    In = np.identity(n)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis.T * M.T, 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M.T, 1, np.array([]), np.array([]))
                    else:
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis.T * (M.T / (Mt @ Mw.T).T), 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M.T / (Mt @ Mw.T).T, 1, np.array([]), np.array([]))

                    TraceKernel = np.trace(Kernel)
                    try:
                        Mh = Mt @ np.linalg.inv(Mt.T @ Mt)
                    except:
                        Mh = Mt @ np.linalg.pinv(Mt.T @ Mt)

                    Mh[np.where(Mh &lt; 0)] = 0
                    for k in range(0, nc):
                        ScaleMt = np.linalg.norm(Mt[:, k])
                        Mt[:, k] = Mt[:, k] / ScaleMt
                        Mh[:, k] = Mh[:, k] * ScaleMt

                    gradMt = Mt @ (Mh.T @ Mh) - Mh
                    gradMh = ((Mt.T @ Mt) @ Mh.T - Mt.T).T
                    initgrad = np.linalg.norm(np.concatenate((gradMt, gradMh), axis=0))
                    tolMh = 1.e-3 * initgrad
                    tolMt = tolMh

            elif (NMFConvex &gt; 0) &amp; (NMFKernel &gt; 1) &amp; (NLKernelApplied == 0):
                NLKernelApplied = 1
                diff0 = 1.e+99
                iIter = NMFMaxInterm + 1
                myStatusBox.init_bar(delay=1)
                cont = 1
                # Calculate centroids
                for k in range(0, nc):
                    Mh[:, k] = Mh[:, k] / np.sum(Mh[:, k])

                Mw = (Mh.T @ M).T
                if (NMFAlgo == 2) or (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Kernel = NMFApplyKernel(Mmis.T * M.T, NMFKernel, Mw, Mt)
                    else:
                        Kernel = NMFApplyKernel(M.T, NMFKernel, Mw, Mt)
                else:
                    if n_Mmis &gt; 0:
                        Kernel = NMFApplyKernel(Mmis.T * (M.T / (Mt @ Mw.T).T), NMFKernel, Mw, Mt)
                    else:
                        Kernel = NMFApplyKernel(M.T / (Mt @ Mw.T).T, NMFKernel, Mw, Mt)

                TraceKernel = np.trace(Kernel)
                try:
                    Mh = Mt @ np.linalg.inv(Mt.T @ Mt)
                except:
                    Mh = Mt @ np.linalg.pinv(Mt.T @ Mt)

                Mh[np.where(Mh &lt; 0)] = 0
                for k in range(0, nc):
                    ScaleMt = np.linalg.norm(Mt[:, k])
                    Mt[:, k] = Mt[:, k] / ScaleMt
                    Mh[:, k] = Mh[:, k] * ScaleMt

                gradMt = Mt @ (Mh.T @ Mh) - Mh
                gradMh = ((Mt.T @ Mt) @ Mh.T - Mt.T).T
                initgrad = np.linalg.norm(np.concatenate((gradMt, gradMh), axis=0))
                tolMh = 1.e-3 * initgrad
                tolMt = tolMh

            if NMFSparseLevel &gt; 0:
                SparseTest = np.zeros((p, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mw[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / p, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
                    lambdaw *= min(1.01 * NMFSparseLevel / PercentZeros, 1.10)
                    iIter = NMFMaxInterm + 1
                    cont = 1

            elif NMFSparseLevel &lt; 0:
                SparseTest = np.zeros((n, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mt[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / n, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
                    lambdat *= min(1.01 * abs(NMFSparseLevel) / PercentZeros, 1.10)
                    iIter = NMFMaxInterm + 1
                    cont = 1

    if NMFFindParts &gt; 0:
        # Make Mt convex
        Mt = M @ Mh
        Mt, Mw, Mh, flagNonconvex, AddMessage, ErrMessage, cancel_pressed = NMFGetConvexScores(Mt, Mw, Mh, flagNonconvex,
                                                                                         AddMessage)
    elif NMFFindCentroids &gt; 0:
        # Calculate row centroids
        for k in range(0, nc):
            ScaleMh = np.sum(Mh[:, k])
            Mh[:, k] = Mh[:, k] / ScaleMh
            Mt[:, k] = Mt[:, k] * ScaleMh

        Mw = (Mh.T @ M).T

    if (NMFKernel &gt; 1) &amp; (NLKernelApplied == 1):
        diff /= TraceKernel / nxp

    return [Mt, Mw, diff, Mh, NMFPriors, flagNonconvex, AddMessage, ErrMessage, cancel_pressed]

def NTFStack(M, Mmis, NBlocks):
    &#34;&#34;&#34;Unfold tensor M
        for future use with NMF
    &#34;&#34;&#34;
    n, p = M.shape
    Mmis = Mmis.astype(np.int)
    n_Mmis = Mmis.shape[0]
    NBlocks = int(NBlocks)

    Mstacked = np.zeros((int(n * p / NBlocks), NBlocks))
    if n_Mmis &gt; 0:
        Mmis_stacked = np.zeros((int(n * p / NBlocks), NBlocks))
    else:
        Mmis_stacked = np.array([])

    for iBlock in range(0, NBlocks):
        for j in range(0, int(p / NBlocks)):
            i1 = j * n
            i2 = i1 + n
            Mstacked[i1:i2, iBlock] = M[:, int(iBlock * p / NBlocks + j)]
            if n_Mmis &gt; 0:
                Mmis_stacked[i1:i2, iBlock] = Mmis[:, int(iBlock * p / NBlocks + j)]

    return [Mstacked, Mmis_stacked]

def NTFSolve(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox):
    &#34;&#34;&#34;Interface to:
            - NTFSolve_simple
            - NTFSolve_conv
    &#34;&#34;&#34;

    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    if n_NMFPriors &gt; 0:
        NMFPriors[NMFPriors &gt; 0] = 1

    if NTFNConv &gt; 0:
        return NTFSolve_conv(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox)
    else:
        return NTFSolve_simple(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NMFPriors, myStatusBox)

def NTFSolve_simple(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NMFPriors, myStatusBox):
    &#34;&#34;&#34;
    Estimate NTF matrices (HALS)
    Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         Mb0: Initial block hand matrix
         nc: NTF rank
         tolerance: Convergence threshold
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
         NMFFixUserRHE: = 1 =&gt; fixed  right hand matrix columns
         NMFFixUserBHE: = 1 =&gt; fixed  block hand matrix columns
         NMFSparseLevel : sparsity level (as defined by Hoyer); +/- = make RHE/LHe sparse
         NTFUnimodal: Apply Unimodal constraint on factoring vectors
         NTFSmooth: Apply Smooth constraint on factoring vectors
         NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
         NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
         NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
         NBlocks: Number of NTF blocks
         NMFPriors: Elements in Mw that should be updated (others remain 0)

    Output:
         Mt: Left hand matrix
         Mw: Right hand matrix
         Mb: Block hand matrix
         diff: objective cost
    
    Reference
    ---------

    A. Cichocki, P.H.A.N. Anh-Huym, Fast local algorithms for large scale nonnegative matrix and tensor factorizations,
        IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 92 (3) (2009) 708–721.

    &#34;&#34;&#34;

    cancel_pressed = 0


    n, p0 = M.shape
    n_Mmis = Mmis.shape[0]
    nc = int(nc)
    NBlocks = int(NBlocks)
    p = int(p0 / NBlocks)
    nxp = int(n * p)
    nxp0 = int(n * p0)
    Mt = np.copy(Mt0)
    Mw = np.copy(Mw0)
    Mb = np.copy(Mb0)
    #     StepIter = math.ceil(MaxIterations/10)
    StepIter = 1
    pbar_step = 100 * StepIter / MaxIterations
 
    IDBlockp = np.arange(0, (NBlocks - 1) * p + 1, p)
    A = np.zeros(n)
    B = np.zeros(p)
    C = np.zeros(NBlocks)

    # Compute Residual tensor
    Mfit = np.zeros((n, p0))
    for k in range(0, nc):
        if NBlocks &gt; 1:
            for iBlock in range(0, NBlocks):
                Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * np.reshape(Mt[:, k], (n, 1)) @ np.reshape(
                    Mw[:, k], (1, p))
        else:
            Mfit[:, IDBlockp[0]:IDBlockp[0] + p] += np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))

    denomt = np.zeros(n)
    denomw = np.zeros(p)
    denomBlock = np.zeros((NBlocks, nc))
    Mt2 = np.zeros(n)
    Mw2 = np.zeros(p)
    MtMw = np.zeros(nxp)
    denomCutoff = .1

    if n_Mmis &gt; 0:
        Mres = (M - Mfit) * Mmis
    else:
        Mres = M - Mfit

    myStatusBox.init_bar(delay=1)

    # Loop
    cont = 1
    iIter = 0
    diff0 = 1.e+99
    Mpart = np.zeros((n, p0))
    # alpha = NMFSparseLevel
    alpha = NMFSparseLevel * .8
    PercentZeros = 0
    iterSparse = 0
    
    while (cont &gt; 0) &amp; (iIter &lt; MaxIterations):
        for k in range(0, nc):
            NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
                NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                denomBlock, NTFBlockComponents, C, Mfit, NMFPriors = \
            NTFUpdate(NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
                NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                denomBlock, NTFBlockComponents, C, Mfit, NMFPriors)
                       
        if iIter % StepIter == 0:
            # Check convergence
            diff = np.linalg.norm(Mres) ** 2 / nxp0
            if (diff0 - diff) / diff0 &lt; tolerance:
                cont = 0                    
            else:
                if diff &gt; diff0:
                    myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR does not improve&#34;)

                diff0 = diff

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)

            if NMFSparseLevel != 0:
                Status = Status + &#39;; Achieved sparsity: &#39; + str(round(PercentZeros, 2)) + &#39;; alpha: &#39; + str(
                    round(alpha, 2))
                if LogIter == 1:
                    myStatusBox.myPrint(Status)

            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [np.array([]), Mt, Mw, Mb, Mres, cancel_pressed]

            if LogIter == 1:
                myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))

        iIter += 1

        if (cont == 0) | (iIter == MaxIterations):
            # if NMFSparseLevel &gt; 0:
            #     SparseTest = np.zeros((p, 1))
            #     for k in range(0, nc):
            #         SparseTest[np.where(Mw[:, k] &gt; 0)] = 1

            #     PercentZeros0 = PercentZeros
            #     n_SparseTest = np.where(SparseTest == 0)[0].size
            #     PercentZeros = max(n_SparseTest / p, .01)
            #     if PercentZeros == PercentZeros0:
            #         iterSparse += 1
            #     else:
            #         iterSparse = 0

            #     if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
            #         alpha *= min(1.01 * NMFSparseLevel / PercentZeros, 1.01)
            #         if alpha &lt; .99:
            #             iIter = 1
            #             cont = 1

            # elif NMFSparseLevel &lt; 0:
            #     SparseTest = np.zeros((n, 1))
            #     for k in range(0, nc):
            #         SparseTest[np.where(Mt[:, k] &gt; 0)] = 1

            #     PercentZeros0 = PercentZeros
            #     n_SparseTest = np.where(SparseTest == 0)[0].size
            #     PercentZeros = max(n_SparseTest / n, .01)
            #     if PercentZeros == PercentZeros0:
            #         iterSparse += 1
            #     else:
            #         iterSparse = 0

            #     if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
            #         alpha *= min(1.01 * abs(NMFSparseLevel) / PercentZeros, 1.01)
            #         if abs(alpha) &lt; .99:
            #             iIter = 1
            #             cont = 1
  
            if NMFSparseLevel &gt; 0:
                SparseTest = np.zeros((nc, 1))
                PercentZeros0 = PercentZeros
                for k in range(0, nc):
                    SparseTest[k] = np.where(Mw[:, k] == 0)[0].size
                
                PercentZeros = np.mean(SparseTest) / p
                if PercentZeros &lt; PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.05 * NMFSparseLevel / PercentZeros, 1.1)
                    if alpha &lt; 1:
                        iIter = 1
                        cont = 1

            elif NMFSparseLevel &lt; 0:
                SparseTest = np.zeros((nc, 1))
                PercentZeros0 = PercentZeros
                for k in range(0, nc):
                    SparseTest[k] = np.where(Mw[:, k] == 0)[0].size
                
                PercentZeros = np.mean(SparseTest) / n
                if PercentZeros &lt; PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.05 * abs(NMFSparseLevel) / PercentZeros, 1.1)
                    if abs(alpha) &lt; 1:
                        iIter = 1
                        cont = 1

    if (n_Mmis &gt; 0) &amp; (NMFFixUserBHE == 0):
        Mb *= denomBlock

    return [np.array([]), Mt, Mw, Mb, diff, cancel_pressed]

def NTFSolve_conv(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox):
    &#34;&#34;&#34;Estimate NTF matrices (HALS)
     Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         Mb0: Initial block hand matrix
         nc: NTF rank
         tolerance: Convergence threshold
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
         NMFFixUserRHE: = 1 =&gt; fixed  right hand matrix columns
         NMFFixUserBHE: = 1 =&gt; fixed  block hand matrix columns
         NMFSparseLevel : sparsity level (as defined by Hoyer); +/- = make RHE/LHe sparse
         NTFUnimodal: Apply Unimodal constraint on factoring vectors
         NTFSmooth: Apply Smooth constraint on factoring vectors
         NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
         NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
         NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
         NBlocks: Number of NTF blocks
         NTFNConv: Half-Size of the convolution window on 3rd-dimension of the tensor
         NMFPriors: Elements in Mw that should be updated (others remain 0)

     Output:
         Mt : if NTFNConv &gt; 0 only otherwise empty. Contains sub-components for each phase in convolution window
         Mt_simple: Left hand matrix (sum of columns Mt_conv for each k)
         Mw_simple: Right hand matrix
         Mb_simple: Block hand matrix
         diff: objective cost
    
     Note: 
         This code extends HALS to allow for shifting on the 3rd dimension of the tensor. Suffix &#39;_simple&#39; is added to 
         the non-convolutional components. Convolutional components are named the usual way.

     &#34;&#34;&#34;

    cancel_pressed = 0

    n, p0 = M.shape
    n_Mmis = Mmis.shape[0]
    nc = int(nc)
    NBlocks = int(NBlocks)
    NTFNConv = int(NTFNConv)
    p = int(p0 / NBlocks)
    nxp = int(n * p)
    nxp0 = int(n * p0)
    Mt_simple = np.copy(Mt0)
    Mw_simple = np.copy(Mw0)
    Mb_simple = np.copy(Mb0)
    #     StepIter = math.ceil(MaxIterations/10)
    StepIter = 1
    pbar_step = 100 * StepIter / MaxIterations

    IDBlockp = np.arange(0, (NBlocks - 1) * p + 1, p)
    A = np.zeros(n)
    B = np.zeros(p)
    C = np.zeros(NBlocks)
    MtMw = np.zeros(nxp)
    NTFNConv2 = 2*NTFNConv + 1
    
    #Initialize Mt, Mw, Mb
    Mt = np.repeat(Mt_simple, NTFNConv2, axis=1) / NTFNConv2
    Mw = np.repeat(Mw_simple, NTFNConv2, axis=1)
    Mb = np.repeat(Mb_simple, NTFNConv2, axis=1)

    for k3 in range(0, nc):
        n_shift = -NTFNConv - 1
        for k2 in range(0, NTFNConv2):
            n_shift += 1
            k = k3*NTFNConv2+k2
            Mb[:,k] = shift(Mb_simple[:, k3], n_shift)

    # Initialize Residual tensor
    Mfit = np.zeros((n, p0))
    for k3 in range(0, nc):
        for k2 in range(0, NTFNConv2):
            k = k3*NTFNConv2+k2
            for iBlock in range(0, NBlocks):
                Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock,k] * \
                    np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))

    denomt = np.zeros(n)
    denomw = np.zeros(p)
    denomBlock = np.zeros((NBlocks, nc))
    Mt2 = np.zeros(n)
    Mw2 = np.zeros(p)
    denomCutoff = .1

    if n_Mmis &gt; 0:
        Mres = (M - Mfit) * Mmis
    else:
        Mres = M - Mfit

    myStatusBox.init_bar(delay=1)

    # Loop
    cont = 1
    iIter = 0
    diff0 = 1.e+99
    Mpart = np.zeros((n, p0)) 
    alpha = NMFSparseLevel
    alpha_blocks = 0
    PercentZeros = 0
    iterSparse = 0

    while (cont &gt; 0) &amp; (iIter &lt; MaxIterations):
        for k3 in range(0, nc):
            for k2 in range(0, NTFNConv2):
                k = k3*NTFNConv2+k2
                NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                    NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
                    NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                    denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                    denomBlock, NTFBlockComponents, C, Mfit, NMFPriors = \
                NTFUpdate(NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                    NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha, \
                    NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                    denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                    denomBlock, NTFBlockComponents, C, Mfit, NMFPriors)
            
            #Update Mt_simple, Mw_simple &amp; Mb_simple
            k = k3*NTFNConv2+NTFNConv
            Mt_simple[:, k3] = Mt[:, k]
            Mw_simple[:, k3] = Mw[:, k]
            Mb_simple[:, k3] = Mb[:, k]

            # Update Mw &amp; Mb
            Mw[:,:] = np.repeat(Mw_simple, NTFNConv2, axis=1)
            n_shift = -NTFNConv - 1
            for k2 in range(0, NTFNConv2):
                n_shift += 1
                k = k3*NTFNConv2+k2
                Mb[:,k] = shift(Mb_simple[:, k3], n_shift)
            
        if iIter % StepIter == 0:
            # Check convergence
            diff = np.linalg.norm(Mres) ** 2 / nxp0
            if (diff0 - diff) / diff0 &lt; tolerance:
                cont = 0
            else:
                diff0 = diff

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)

            if NMFSparseLevel != 0:
                Status = Status + &#39;; Achieved sparsity: &#39; + str(round(PercentZeros, 2)) + &#39;; alpha: &#39; + str(
                    round(alpha, 2))
                if LogIter == 1:
                    myStatusBox.myPrint(Status)

            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [Mt, Mt_simple, Mw_simple, Mb_simple, cancel_pressed]

            if LogIter == 1:
                myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))

        iIter += 1

        if (cont == 0) | (iIter == MaxIterations):
            if NMFSparseLevel &gt; 0:
                SparseTest = np.zeros((p, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mw[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / p, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.01 * NMFSparseLevel / PercentZeros, 1.01)
                    if alpha &lt; .99:
                        iIter = 1
                        cont = 1

            elif NMFSparseLevel &lt; 0:
                SparseTest = np.zeros((n, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mt[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / n, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.01 * abs(NMFSparseLevel) / PercentZeros, 1.01)
                    if abs(alpha) &lt; .99:
                        iIter = 1
                        cont = 1

    if (n_Mmis &gt; 0) &amp; (NMFFixUserBHE == 0):
        Mb *= denomBlock

    return [Mt, Mt_simple, Mw_simple, Mb_simple, diff, cancel_pressed]

def NTFSolveFast(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, precision, LogIter, Status0, MaxIterations, NMFFixUserLHE,
                 NMFFixUserRHE, NMFFixUserBHE, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents,
                 NBlocks, myStatusBox):
    &#34;&#34;&#34;Estimate NTF matrices (fast HALS)
     Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         Mb0: Initial block hand matrix
         nc: NTF rank
         tolerance: Convergence threshold
         precision: Replace 0-values in multiplication rules
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFFixUserLHE: fix left hand matrix columns: = 1, else = 0
         NMFFixUserRHE: fix  right hand matrix columns: = 1, else = 0
         NMFFixUserBHE: fix  block hand matrix columns: = 1, else = 0
         NTFUnimodal: Apply Unimodal constraint on factoring vectors
         NTFSmooth: Apply Smooth constraint on factoring vectors
         NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
         NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
         NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
         NBlocks: Number of NTF blocks
     Output:
         Mt: Left hand matrix
         Mw: Right hand matrix
         Mb: Block hand matrix
         diff: objective cost

     Note: This code does not support missing values, nor sparsity constraint

     &#34;&#34;&#34;
    Mres = np.array([])
    cancel_pressed = 0

    n, p0 = M.shape
    n_Mmis = Mmis.shape[0]
    nc = int(nc)
    NBlocks = int(NBlocks)
    p = int(p0 / NBlocks)
    n0 = int(n * NBlocks)
    nxp = int(n * p)
    nxp0 = int(n * p0)
    Mt = np.copy(Mt0)
    Mw = np.copy(Mw0)
    Mb = np.copy(Mb0)
    StepIter = math.ceil(MaxIterations / 10)
    pbar_step = 100 * StepIter / MaxIterations

    IDBlockn = np.arange(0, (NBlocks - 1) * n + 1, n)
    IDBlockp = np.arange(0, (NBlocks - 1) * p + 1, p)
    A = np.zeros(n)
    B = np.zeros(p)
    C = np.zeros(NBlocks)

    if NMFFixUserBHE &gt; 0:
        NormBHE = True
        if NMFFixUserRHE == 0:
            NormLHE = True
            NormRHE = False
        else:
            NormLHE = False
            NormRHE = True
    else:
            NormBHE = False
            NormLHE = True
            NormRHE = True

    for k in range(0, nc):
        if (NMFFixUserLHE &gt; 0) &amp; NormLHE:
            norm = np.linalg.norm(Mt[:, k])
            if norm &gt; 0:
                Mt[:, k] /= norm
    
        if (NMFFixUserRHE &gt; 0) &amp; NormRHE:
            norm = np.linalg.norm(Mw[:, k])
            if norm &gt; 0:
                Mw[:, k] /= norm
            
        if (NMFFixUserBHE &gt; 0) &amp; NormBHE:
            norm = np.linalg.norm(Mb[:, k])
            if norm &gt; 0:
                Mb[:, k] /= norm
    
    # Normalize factors to unit length
    #    for k in range(0, nc):
    #        ScaleMt = np.linalg.norm(Mt[:, k])
    #        Mt[:, k] /= ScaleMt
    #        ScaleMw = np.linalg.norm(Mw[:, k])
    #        Mw[:, k] /= ScaleMw
    #        Mb[:, k] *= (ScaleMt * ScaleMw)

    # Initialize T1
    Mt2 = Mt.T @ Mt
    Mt2[Mt2 == 0] = precision
    Mw2 = Mw.T @ Mw
    Mw2[Mw2 == 0] = precision
    Mb2 = Mb.T @ Mb
    Mb2[Mb2 == 0] = precision
    T1 = Mt2 * Mw2 * Mb2
    T2t = np.zeros((n, nc))
    T2w = np.zeros((p, nc))
    T2Block = np.zeros((NBlocks, nc))

    # Transpose M by block once for all
    M2 = np.zeros((p, n0))

    Mfit = np.zeros((n, p0))
    if n_Mmis &gt; 0:
        denomt = np.zeros(n)
        denomw = np.zeros(p)
        denomBlock = np.ones((NBlocks, nc))
        MxMmis2 = np.zeros((p, n0))
        denomCutoff = .1

    myStatusBox.init_bar(delay=1)

    # Loop
    cont = 1
    iIter = 0
    diff0 = 1.e+99


    for iBlock in range(0, NBlocks):
        M2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] = M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T
        if n_Mmis &gt; 0:
            MxMmis2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] = (M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] * \
                                                                 Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p]).T

    if n_Mmis &gt; 0:
        MxMmis = M * Mmis

    while (cont &gt; 0) &amp; (iIter &lt; MaxIterations):
        if n_Mmis &gt; 0:
            Gamma = np.diag((denomBlock*Mb).T @ (denomBlock*Mb))
        else:
            Gamma = np.diag(Mb.T @ Mb)

        if NMFFixUserLHE == 0:
            # Update Mt
            T2t[:,:] = 0
            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    denomt[:] = 0
                    Mwn = np.repeat(Mw[:, k, np.newaxis] ** 2, n, axis=1)
                    for iBlock in range(0, NBlocks):
                        # Broadcast missing cells into Mw to calculate Mw.T * Mw
                        denomt += Mb[iBlock, k]**2 * np.sum(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T * Mwn, axis = 0)

                    denomt /= np.max(denomt)
                    denomt[denomt &lt; denomCutoff] = denomCutoff
                    for iBlock in range(0, NBlocks):
                        T2t[:, k] += MxMmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw[:, k] * Mb[iBlock, k]

                    T2t[:, k] /= denomt
                else:
                    for iBlock in range(0, NBlocks):
                        T2t[:, k] += M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw[:, k] * Mb[iBlock, k]

            Mt2 = Mt.T @ Mt
            Mt2[Mt2 == 0] = precision
            T3 = T1 / Mt2

            for k in range(0, nc):
                Mt[:, k] = Gamma[k] * Mt[:, k] + T2t[:, k] - Mt @ T3[:, k]
                Mt[np.where(Mt[:, k] &lt; 0), k] = 0

                if (NTFUnimodal &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #                 Enforce unimodal distribution
                    tmax = np.argmax(Mt[:, k])
                    for i in range(tmax + 1, n):
                        Mt[i, k] = min(Mt[i - 1, k], Mt[i, k])

                    for i in range(tmax - 1, -1, -1):
                        Mt[i, k] = min(Mt[i + 1, k], Mt[i, k])

                if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #             Smooth distribution
                    A[0] = .75 * Mt[0, k] + .25 * Mt[1, k]
                    A[n - 1] = .25 * Mt[n - 2, k] + .75 * Mt[n - 1, k]
                    for i in range(1, n - 1):
                        A[i] = .25 * Mt[i - 1, k] + .5 * Mt[i, k] + .25 * Mt[i + 1, k]

                    Mt[:, k] = A

                if NormLHE:
                    Mt[:, k] /= np.linalg.norm(Mt[:, k])

            Mt2 = Mt.T @ Mt
            Mt2[Mt2 == 0] = precision
            T1 = T3 * Mt2

        if NMFFixUserRHE == 0:
            # Update Mw
            T2w[:,:] = 0
            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    denomw[:] = 0
                    Mtp = np.repeat(Mt[:, k, np.newaxis] ** 2, p, axis=1)
                    for iBlock in range(0, NBlocks):
                        # Broadcast missing cells into Mw to calculate Mt.T * Mt
                        denomw += Mb[iBlock, k]**2 * np.sum(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] * Mtp, axis = 0)

                    denomw /= np.max(denomw)
                    denomw[denomw &lt; denomCutoff] = denomCutoff
                    for iBlock in range(0, NBlocks):
                        T2w[:, k] += MxMmis2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] @ Mt[:, k] * Mb[iBlock, k]

                    T2w[:, k] /= denomw
                else:
                    for iBlock in range(0, NBlocks):
                        T2w[:, k] += M2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] @ Mt[:, k] * Mb[iBlock, k]

            Mw2 = Mw.T @ Mw
            Mw2[Mw2 == 0] = precision
            T3 = T1 / Mw2

            for k in range(0, nc):
                Mw[:, k] = Gamma[k] * Mw[:, k] + T2w[:, k] - Mw @ T3[:, k]
                Mw[np.where(Mw[:, k] &lt; 0), k] = 0

                if (NTFUnimodal &gt; 0) &amp; (NTFRightComponents &gt; 0):
                    #                 Enforce unimodal distribution
                    wmax = np.argmax(Mw[:, k])
                    for j in range(wmax + 1, p):
                        Mw[j, k] = min(Mw[j - 1, k], Mw[j, k])

                    for j in range(wmax - 1, -1, -1):
                        Mw[j, k] = min(Mw[j + 1, k], Mw[j, k])

                if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #             Smooth distribution
                    B[0] = .75 * Mw[0, k] + .25 * Mw[1, k]
                    B[p - 1] = .25 * Mw[p - 2, k] + .75 * Mw[p - 1, k]
                    for j in range(1, p - 1):
                        B[j] = .25 * Mw[j - 1, k] + .5 * Mw[j, k] + .25 * Mw[j + 1, k]

                    Mw[:, k] = B

                if NormRHE:
                    Mw[:, k] /= np.linalg.norm(Mw[:, k])

            Mw2 = Mw.T @ Mw
            Mw2[Mw2 == 0] = precision
            T1 = T3 * Mw2

        if NMFFixUserBHE == 0:
            # Update Mb
            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    for iBlock in range(0, NBlocks):
                        # Broadcast missing cells into Mb to calculate Mb.T * Mb
                        denomBlock[iBlock, k] = np.sum(np.reshape(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp) *
                                np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp)**2, axis=0)

                    maxdenomBlock = np.max(denomBlock[:, k])
                    denomBlock[denomBlock[:, k] &lt; denomCutoff * maxdenomBlock] = denomCutoff * maxdenomBlock
                    for iBlock in range(0, NBlocks):
                        T2Block[iBlock, k] = np.reshape(MxMmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp).T @ \
                                        (np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp)) / denomBlock[iBlock, k]

                else:
                    for iBlock in range(0, NBlocks):
                        T2Block[iBlock, k] = np.reshape(M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp).T @ \
                                        (np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp))

            Mb2 = Mb.T @ Mb
            Mb2[Mb2 == 0] = precision
            T3 = T1 / Mb2

            for k in range(0, nc):
                Mb[:, k] = Mb[:, k] + T2Block[:, k] - Mb @ T3[:, k]
                Mb[np.where(Mb[:, k] &lt; 0), k] = 0

                if (NTFUnimodal &gt; 0) &amp; (NTFBlockComponents &gt; 0):
                    #                 Enforce unimodal distribution
                    bmax = np.argmax(Mb[:, k])
                    for iBlock in range(bmax + 1, NBlocks):
                        Mb[iBlock, k] = min(Mb[iBlock - 1, k], Mb[iBlock, k])

                    for iBlock in range(bmax - 1, -1, -1):
                        Mb[iBlock, k] = min(Mb[iBlock + 1, k], Mb[iBlock, k])

                if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #             Smooth distribution
                    C[0] = .75 * Mb[0, k] + .25 * Mb[1, k]
                    C[NBlocks - 1] = .25 * Mb[NBlocks - 2, k] + .75 * Mb[NBlocks - 1, k]
                    for iBlock in range(1, NBlocks - 1):
                        C[iBlock] = .25 * Mb[iBlock - 1, k] + .5 * Mb[iBlock, k] + .25 * Mb[iBlock + 1, k]

                    Mb[:, k] = C

            Mb2 = Mb.T @ Mb
            Mb2[Mb2 == 0] = precision
            T1 = T3 * Mb2

        if iIter % StepIter == 0:
            # Update residual tensor
            Mfit[:,:] = 0

            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    for iBlock in range(0, NBlocks):
                        #Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += denomBlock[iBlock, k] * Mb[iBlock, k] * (
                        Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * (
                        np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p)))

                    Mres = (M - Mfit) * Mmis
                else:
                    for iBlock in range(0, NBlocks):
                        Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * (
                                np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p)))

                    Mres = (M - Mfit)

            # Check convergence
            diff = np.linalg.norm(Mres) ** 2 / nxp0
            if (diff0 - diff) / diff0 &lt; tolerance:
                cont = 0
            else:
                diff0 = diff

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)
            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [Mt, Mw, Mb, Mres, cancel_pressed]

            if LogIter == 1:
                myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))

        iIter += 1

    if n_Mmis &gt; 0:
        Mb *= denomBlock

    return [Mt, Mw, Mb, diff, cancel_pressed]

def NTFUpdate(NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
        NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha, \
        NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
        denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
        denomBlock, NTFBlockComponents, C, Mfit, NMFPriors):
    &#34;&#34;&#34;Core updating code called by NTFSolve_simple &amp; NTF Solve_conv
    Input:
        All variables in the calling function used in the function 
    Output:
        Same as Input
    &#34;&#34;&#34;

    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    # Compute kth-part
    if NBlocks &gt; 1:
        for iBlock in range(0, NBlocks):
            Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] = Mb[iBlock, k] * \
                np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))
    else:
        Mpart[:, IDBlockp[0]:IDBlockp[0] + p] = np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))

    if n_Mmis &gt; 0:
        Mpart *= Mmis

    Mpart += Mres 

    if NMFFixUserBHE &gt; 0:
        NormBHE = True
        if NMFFixUserRHE == 0:
            NormLHE = True
            NormRHE = False
        else:
            NormLHE = False
            NormRHE = True
    else:
            NormBHE = False
            NormLHE = True
            NormRHE = True

    if (NMFFixUserLHE &gt; 0) &amp; NormLHE:
        norm = np.linalg.norm(Mt[:, k])
        if norm &gt; 0:
            Mt[:, k] /= norm

    if (NMFFixUserRHE &gt; 0) &amp; NormRHE:
        norm = np.linalg.norm(Mw[:, k])
        if norm &gt; 0:
            Mw[:, k] /= norm
        
    if (NMFFixUserBHE &gt; 0) &amp; NormBHE &amp; (NBlocks &gt; 1):
        norm = np.linalg.norm(Mb[:, k])
        if norm &gt; 0:
            Mb[:, k] /= norm

    if NMFFixUserLHE == 0:
        # Update Mt
        Mt[:, k] = 0
        if NBlocks &gt; 1:
            for iBlock in range(0, NBlocks):
                Mt[:, k] += Mb[iBlock, k] * Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw[:, k]
        else:
            Mt[:, k] += Mpart[:, IDBlockp[0]:IDBlockp[0] + p] @ Mw[:, k]

        if n_Mmis &gt; 0:
            denomt[:] = 0
            Mw2[:] = Mw[:, k] ** 2
            if NBlocks &gt; 1:
                for iBlock in range(0, NBlocks):
                    # Broadcast missing cells into Mw to calculate Mw.T * Mw
                    denomt += Mb[iBlock, k]**2 * Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw2
            else:
                denomt += Mmis[:, IDBlockp[0]:IDBlockp[0] + p] @ Mw2

            denomt /= np.max(denomt)
            denomt[denomt &lt; denomCutoff] = denomCutoff
            Mt[:, k] /= denomt               

        Mt[Mt[:, k] &lt; 0, k] = 0
        if alpha &lt; 0:
            Mt[:, k] = sparse_opt(Mt[:, k], -alpha, False)
         
        if (NTFUnimodal &gt; 0) &amp; (NTFLeftComponents &gt; 0):
            #                 Enforce unimodal distribution
            tmax = np.argmax(Mt[:, k])
            for i in range(tmax + 1, n):
                Mt[i, k] = min(Mt[i - 1, k], Mt[i, k])

            for i in range(tmax - 1, -1, -1):
                Mt[i, k] = min(Mt[i + 1, k], Mt[i, k])

        if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
            #             Smooth distribution
            A[0] = .75 * Mt[0, k] + .25 * Mt[1, k]
            A[n - 1] = .25 * Mt[n - 2, k] + .75 * Mt[n - 1, k]
            for i in range(1, n - 1):
                A[i] = .25 * Mt[i - 1, k] + .5 * Mt[i, k] + .25 * Mt[i + 1, k]

            Mt[:, k] = A

        if NormLHE:
            norm = np.linalg.norm(Mt[:, k])
            if norm &gt; 0:
                Mt[:, k] /= norm

    if NMFFixUserRHE == 0:
        # Update Mw
        
        Mw[:, k] = 0
        if NBlocks &gt; 1:
            for iBlock in range(0, NBlocks):
                Mw[:, k] += Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T @ Mt[:, k] * Mb[iBlock, k]
        else:
            Mw[:, k] += Mpart[:, IDBlockp[0]:IDBlockp[0] + p].T @ Mt[:, k]

        if n_Mmis &gt; 0:
            denomw[:] = 0
            Mt2[:] = Mt[:, k] ** 2
            if NBlocks &gt; 1:
                for iBlock in range(0, NBlocks):
                    # Broadcast missing cells into Mw to calculate Mt.T * Mt
                    denomw += Mb[iBlock, k] ** 2 * Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T @ Mt2
            else:
                denomw += Mmis[:, IDBlockp[0]:IDBlockp[0] + p].T @ Mt2

            denomw /= np.max(denomw)
            denomw[denomw &lt; denomCutoff] = denomCutoff
            Mw[:, k] /= denomw

        Mw[Mw[:, k] &lt; 0, k] = 0

        if alpha &gt; 0:
            Mw[:, k] = sparse_opt(Mw[:, k], alpha, False)

        if (NTFUnimodal &gt; 0) &amp; (NTFRightComponents &gt; 0):
            #Enforce unimodal distribution
            wmax = np.argmax(Mw[:, k])
            for j in range(wmax + 1, p):
                Mw[j, k] = min(Mw[j - 1, k], Mw[j, k])

            for j in range(wmax - 1, -1, -1):
                Mw[j, k] = min(Mw[j + 1, k], Mw[j, k])

        if (NTFSmooth &gt; 0) &amp; (NTFRightComponents &gt; 0):
            #             Smooth distribution
            B[0] = .75 * Mw[0, k] + .25 * Mw[1, k]
            B[p - 1] = .25 * Mw[p - 2, k] + .75 * Mw[p - 1, k]
            for j in range(1, p - 1):
                B[j] = .25 * Mw[j - 1, k] + .5 * Mw[j, k] + .25 * Mw[j + 1, k]

            Mw[:, k] = B

        if n_NMFPriors &gt; 0:
            Mw[:, k] = Mw[:, k] * NMFPriors[:, k]

        if NormRHE:
            norm = np.linalg.norm(Mw[:, k])
            if norm &gt; 0:
                Mw[:, k] /= norm

    if NMFFixUserBHE == 0:
        # Update Mb
        Mb[:, k] = 0
        MtMw[:] = np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp)

        for iBlock in range(0, NBlocks):
            Mb[iBlock, k] = np.reshape(Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp).T @ MtMw

        if n_Mmis &gt; 0:                          
            MtMw[:] = MtMw[:] ** 2
            for iBlock in range(0, NBlocks):
                # Broadcast missing cells into Mb to calculate Mb.T * Mb
                denomBlock[iBlock, k] = np.reshape(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], (1, nxp)) @ MtMw

            maxdenomBlock = np.max(denomBlock[:, k])
            denomBlock[denomBlock[:, k] &lt; denomCutoff * maxdenomBlock] = denomCutoff * maxdenomBlock
            Mb[:, k] /= denomBlock[:, k]

        Mb[Mb[:, k] &lt; 0, k] = 0

        if (NTFUnimodal &gt; 0) &amp; (NTFBlockComponents &gt; 0):
            #                 Enforce unimodal distribution
            bmax = np.argmax(Mb[:, k])
            for iBlock in range(bmax + 1, NBlocks):
                Mb[iBlock, k] = min(Mb[iBlock - 1, k], Mb[iBlock, k])

            for iBlock in range(bmax - 1, -1, -1):
                Mb[iBlock, k] = min(Mb[iBlock + 1, k], Mb[iBlock, k])

        if (NTFSmooth &gt; 0) &amp; (NTFBlockComponents &gt; 0):
            #             Smooth distribution
            C[0] = .75 * Mb[0, k] + .25 * Mb[1, k]
            C[NBlocks - 1] = .25 * Mb[NBlocks - 2, k] + .75 * Mb[NBlocks - 1, k]
            for iBlock in range(1, NBlocks - 1):
                C[iBlock] = .25 * Mb[iBlock - 1, k] + .5 * Mb[iBlock, k] + .25 * Mb[iBlock + 1, k]

            Mb[:, k] = C
        
        if NormBHE:
            norm = np.linalg.norm(Mb[:, k])
            if norm &gt; 0:
                Mb[:, k] /= norm

    # Update residual tensor
    Mfit[:,:] = 0
    if NBlocks &gt; 1:
        for iBlock in range(0, NBlocks):
            Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * \
                np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))
    else:
        Mfit[:, IDBlockp[0]:IDBlockp[0] + p] += np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))


    if n_Mmis &gt; 0:
        Mres[:,:] = (Mpart - Mfit) * Mmis
    else:
        Mres[:,:] = Mpart - Mfit

    return NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
            NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
            NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
            denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
            denomBlock, NTFBlockComponents, C, Mfit, NMFPriors
 </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="nmtf.modules.nmtf_core.NMFApplyKernel"><code class="name flex">
<span>def <span class="ident">NMFApplyKernel</span></span>(<span>M, NMFKernel, Mt, Mw)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate kernel (used with convex NMF)</p>
<h2 id="input">Input</h2>
<p>M: Input matrix
NMFKernel: Type of kernel
=-1: linear
= 2: quadratic
= 3: radiant
Mt: Left factoring matrix
Mw: Right factoring matrix</p>
<h2 id="output">Output</h2>
<p>Kernel</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NMFApplyKernel(M, NMFKernel, Mt, Mw):
    &#34;&#34;&#34;Calculate kernel (used with convex NMF)
    Input:
        M: Input matrix
        NMFKernel: Type of kernel
            =-1: linear
            = 2: quadratic
            = 3: radiant
        Mt: Left factoring matrix
        Mw: Right factoring matrix
    Output:
        Kernel
    &#34;&#34;&#34;

    n, p = M.shape
    try:
        p, nc = Mw.shape
    except:
        nc = 0

    if NMFKernel == 1:
        Kernel = M.T @ M
    elif NMFKernel == 2:
        Kernel = (np.identity(p) + M.T @ M) ** 2
    elif NMFKernel == 3:
        Kernel = np.identity(p)
        # Estimate Sigma2
        Sigma2 = 0

        for k1 in range(1, nc):
            for k2 in range(0, k1):
                Sigma2 = max(Sigma2, np.linalg.norm(Mt[:, k1] - Mt[:, k2]) ** 2)

        Sigma2 /= nc
        for j1 in range(1, p):
            for j2 in range(0, j1):
                Kernel[j1, j2] = math.exp(-np.linalg.norm(M[:, j1] - M[:, j2]) ** 2 / Sigma2)
                Kernel[j2, j1] = Kernel[j1, j2]

    return Kernel</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NMFProjGrad"><code class="name flex">
<span>def <span class="ident">NMFProjGrad</span></span>(<span>V, Vmis, W, Hinit, NMFAlgo, lambdax, tol, MaxIterations, NMFPriors)</span>
</code></dt>
<dd>
<div class="desc"><p>Projected gradient
Code and notations adapted from Matlab code, Chih-Jen Lin</p>
<h2 id="input">Input</h2>
<p>V: Input matrix
Vmis: Define missing values (0 = missing cell, 1 = real cell)
W: Left factoring vectors (fixed)
Hinit: Right factoring vectors (initial values)
NMFAlgo: =1,3: Divergence; =2,4: Least squares;
lambdax: Sparseness parameter
=-1: no penalty
&lt; 0: Target percent zeroed rows in H
&gt; 0: Current penalty
tol: Tolerance
MaxIterations: max number of iterations to achieve norm(projected gradient) &lt; tol
NMFPriors: Elements in H that should be updated (others remain 0)</p>
<h2 id="output">Output</h2>
<p>H: Estimated right factoring vectors
tol: Current level of the tolerance
lambdax: Current level of the penalty</p>
<h2 id="reference">Reference</h2>
<p>C.J. Lin (2007) Projected Gradient Methods for Non-negative Matrix Factorization
Neural Comput. 2007 Oct;19(10):2756-79.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NMFProjGrad(V, Vmis, W, Hinit, NMFAlgo, lambdax, tol, MaxIterations, NMFPriors):
    &#34;&#34;&#34;Projected gradient
    Code and notations adapted from Matlab code, Chih-Jen Lin
    Input:
        V: Input matrix
        Vmis: Define missing values (0 = missing cell, 1 = real cell)
        W: Left factoring vectors (fixed)
        Hinit: Right factoring vectors (initial values)
        NMFAlgo: =1,3: Divergence; =2,4: Least squares;
        lambdax: Sparseness parameter
            =-1: no penalty
            &lt; 0: Target percent zeroed rows in H
            &gt; 0: Current penalty
        tol: Tolerance
        MaxIterations: max number of iterations to achieve norm(projected gradient) &lt; tol
        NMFPriors: Elements in H that should be updated (others remain 0)
    Output:
        H: Estimated right factoring vectors
        tol: Current level of the tolerance
        lambdax: Current level of the penalty
    
    Reference
    ---------

    C.J. Lin (2007) Projected Gradient Methods for Non-negative Matrix Factorization
    Neural Comput. 2007 Oct;19(10):2756-79.

    &#34;&#34;&#34;
    H = Hinit
    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    n_Vmis = Vmis.shape[0]
    n, p = np.shape(V)
    n, nc = np.shape(W)
    alpha = 1

    if (NMFAlgo == 2) or (NMFAlgo == 4):
        beta = .1
        if n_Vmis &gt; 0:
            WtV = W.T @ (V * Vmis)
        else:
            WtV = W.T @ V
            WtW = W.T @ W
    else:
        beta = .1
        if n_Vmis &gt; 0:
            WtWH = W.T @ Vmis
        else:
            WtWH = W.T @ np.ones((n, p))

    if (lambdax &lt; 0) &amp; (lambdax != -1):
        H0 = H

    restart = True
    while restart:
        for iIter in range(1, MaxIterations + 1):
            addPenalty = 0
            if lambdax != -1:
                addPenalty = 1

            if (NMFAlgo == 2) or (NMFAlgo == 4):
                if n_Vmis &gt; 0:
                    WtWH = W.T @ ((W @ H) * Vmis)
                else:
                    WtWH = WtW @ H
            else:
                if n_Vmis &gt; 0:
                    WtV = W.T @ ((V * Vmis) / (W @ H))
                else:
                    WtV = W.T @ (V / (W @ H))

            if lambdax &gt; 0:
                grad = WtWH - WtV + lambdax
            else:
                grad = WtWH - WtV

            projgrad = np.linalg.norm(grad[(grad &lt; 0) | (H &gt; 0)])

            if projgrad &gt;= tol:
                # search step size
                for inner_iter in range(1, 21):
                    Hn = H - alpha * grad
                    Hn[np.where(Hn &lt; 0)] = 0
                    if n_NMFPriors &gt; 0:
                        Hn = Hn * NMFPriors

                    d = Hn - H
                    gradd = np.sum(grad * d)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Vmis &gt; 0:
                            dQd = np.sum((W.T @ ((W @ d) * Vmis)) * d)
                        else:
                            dQd = np.sum((WtW @ d) * d)
                    else:
                        if n_Vmis &gt; 0:
                            dQd = np.sum((W.T @ ((W @ d) * (Vmis / (W @ H)))) * d)
                        else:
                            dQd = np.sum((W.T @ ((W @ d) / (W @ H))) * d)

                    suff_decr = (0.99 * gradd + 0.5 * dQd &lt; 0)
                    if inner_iter == 1:
                        decr_alpha = not suff_decr
                        Hp = H

                    if decr_alpha:
                        if suff_decr:
                            H = Hn
                            break
                        else:
                            alpha = alpha * beta
                    else:
                        if (suff_decr == False) | (np.where(Hp != Hn)[0].size == 0):
                            H = Hp
                            break
                        else:
                            alpha = alpha / beta
                            Hp = Hn
                # End for (inner_iter

                if (lambdax &lt; 0) &amp; addPenalty:
                    # Initialize penalty
                    lambdax = percentile_exc(H[np.where(H &gt; 0)], -lambdax * 100)
                    H = H0
                    alpha = 1
            else:  # projgrad &lt; tol
                if (iIter == 1) &amp; (projgrad &gt; 0):
                    tol /= 10
                else:
                    restart = False

                break
            #       End if projgrad

            if iIter == MaxIterations:
                restart = False
        #   End For iIter

    H = H.T
    return [H, tol, lambdax]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NMFProjGradKernel"><code class="name flex">
<span>def <span class="ident">NMFProjGradKernel</span></span>(<span>Kernel, V, Vmis, W, Hinit, NMFAlgo, tol, MaxIterations, NMFPriors)</span>
</code></dt>
<dd>
<div class="desc"><p>Projected gradient, kernel version
Code and notations adapted from Matlab code, Chih-Jen Lin</p>
<h2 id="input">Input</h2>
<p>Kernel: Kernel used
V: Input matrix
Vmis: Define missing values (0 = missing cell, 1 = real cell)
W: Left factoring vectors (fixed)
Hinit: Right factoring vectors (initial values)
NMFAlgo: =1,3: Divergence; =2,4: Least squares;
tol: Tolerance
MaxIterations: max number of iterations to achieve norm(projected gradient) &lt; tol
NMFPriors: Elements in H that should be updated (others remain 0)</p>
<h2 id="output">Output</h2>
<p>H: Estimated right factoring vectors
tol: Current level of the tolerance</p>
<h2 id="reference">Reference</h2>
<p>C.J. Lin (2007) Projected Gradient Methods for Non-negative Matrix Factorization
Neural Comput. 2007 Oct;19(10):2756-79.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NMFProjGradKernel(Kernel, V, Vmis, W, Hinit, NMFAlgo, tol, MaxIterations, NMFPriors):
    &#34;&#34;&#34;Projected gradient, kernel version
    Code and notations adapted from Matlab code, Chih-Jen Lin
    Input:
        Kernel: Kernel used
        V: Input matrix
        Vmis: Define missing values (0 = missing cell, 1 = real cell)
        W: Left factoring vectors (fixed)
        Hinit: Right factoring vectors (initial values)
        NMFAlgo: =1,3: Divergence; =2,4: Least squares;
        tol: Tolerance
        MaxIterations: max number of iterations to achieve norm(projected gradient) &lt; tol
        NMFPriors: Elements in H that should be updated (others remain 0)
    Output:
        H: Estimated right factoring vectors
        tol: Current level of the tolerance
    
    Reference
    ---------

    C.J. Lin (2007) Projected Gradient Methods for Non-negative Matrix Factorization
        Neural Comput. 2007 Oct;19(10):2756-79.

    &#34;&#34;&#34;
    H = Hinit.T
    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    n_Vmis = Vmis.shape[0]
    n, p = np.shape(V)
    p, nc = np.shape(W)
    alpha = 1
    VW = V @ W

    if (NMFAlgo == 2) or (NMFAlgo == 4):
        beta = .1
        if n_Vmis &gt; 0:
            WtV = VW.T @ (V * Vmis)
        else:
            WtV = W.T @ Kernel
            WtW = W.T @ Kernel @ W
    else:
        beta = .1
        MaxIterations = round(MaxIterations/10)
        if n_Vmis &gt; 0:
            WtWH = VW.T @ Vmis
        else:
            WtWH = VW.T @ np.ones((n, p))

    restart = True
    while restart:
        for iIter in range(1, MaxIterations + 1):
            if (NMFAlgo == 2) or (NMFAlgo == 4):
                if n_Vmis &gt; 0:
                    WtWH = VW.T @ ((VW @ H) * Vmis)
                else:
                    WtWH = WtW @ H
            else:
                if n_Vmis &gt; 0:
                    WtV = VW.T @ ((V * Vmis) / (VW @ H))
                else:
                    WtV = VW.T @ (V / (VW @ H))

            grad = WtWH - WtV
            projgrad = np.linalg.norm(grad[(grad &lt; 0) | (H &gt; 0)])
            if projgrad &gt;= tol:
                # search step size
                for inner_iter in range(1, 21):
                    Hn = H - alpha * grad
                    Hn[np.where(Hn &lt; 0)] = 0
                    if n_NMFPriors &gt; 0:
                        Hn = Hn * NMFPriors

                    d = Hn - H
                    gradd = np.sum(grad * d)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Vmis &gt; 0:
                            dQd = np.sum((VW.T @ ((VW @ d) * Vmis)) * d)
                        else:
                            dQd = np.sum((WtW @ d) * d)
                    else:
                        if n_Vmis &gt; 0:
                            dQd = np.sum((VW.T @ ((VW @ d) * (Vmis / (VW @ H)))) * d)
                        else:
                            dQd = np.sum((VW.T @ ((VW @ d) / (VW @ H))) * d)

                    suff_decr = (0.99 * gradd + 0.5 * dQd &lt; 0)
                    if inner_iter == 1:
                        decr_alpha = not suff_decr
                        Hp = H

                    if decr_alpha:
                        if suff_decr:
                            H = Hn
                            break
                        else:
                            alpha = alpha * beta
                    else:
                        if (suff_decr == False) | (np.where(Hp != Hn)[0].size == 0):
                            H = Hp
                            break
                        else:
                            alpha = alpha / beta
                            Hp = Hn
                # End for (inner_iter
            else:  # projgrad &lt; tol
                if iIter == 1:
                    tol /= 10
                else:
                    restart = False

                break
            #       End if projgrad

            if iIter == MaxIterations:
                restart = False
        #   End For iIter

    H = H.T
    return [H, tol]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NMFReweigh"><code class="name flex">
<span>def <span class="ident">NMFReweigh</span></span>(<span>M, Mt, NMFPriors, AddMessage)</span>
</code></dt>
<dd>
<div class="desc"><p>Overload skewed variables (used with deconvolution only)</p>
<h2 id="input">Input</h2>
<p>M: Input matrix
Mt: Left hand matrix
NMFPriors: priors on right hand matrix</p>
<h2 id="output">Output</h2>
<p>NMFPriors: updated priors</p>
<p>Note: This code is still experimental</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NMFReweigh(M, Mt, NMFPriors, AddMessage):
    &#34;&#34;&#34;Overload skewed variables (used with deconvolution only)
    Input:
         M: Input matrix
         Mt: Left hand matrix
         NMFPriors: priors on right hand matrix
    Output:
         NMFPriors: updated priors

    Note: This code is still experimental

    &#34;&#34;&#34;
    ErrMessage = &#34;&#34;
    n, p = M.shape
    n_NMFPriors, nc = NMFPriors.shape
    NMFPriors[NMFPriors &gt; 0] = 1
    ID = np.where(np.sum(NMFPriors, axis=1) &gt; 1)
    n_ID = ID[0].shape[0]
    if n_ID == p:
        ErrMessage = &#39;Error! All priors are ambiguous.\nYou may uncheck the option in tab irMF+.&#39;
        return [NMFPriors, AddMessage, ErrMessage]

    NMFPriors[ID, :] = 0
    Mweight = np.zeros((p, nc))
    for k in range(0, nc):
        ID = np.where(NMFPriors[:, k] &gt; 0)
        pk = ID[0].shape[0]
        if pk == 0:
            ErrMessage = &#39;Error! Null column in NMF priors (&#39; + str(k+1) + &#39;, pre outlier filtering)&#39;
            return [NMFPriors, AddMessage, ErrMessage]

        Mc = np.zeros((n, p))

        # Exclude variables with outliers
        NInterQuart = 1.5
        for j in range(0, pk):
            Quart75 = percentile_exc(M[:, ID[0][j]], 75)
            Quart25 = percentile_exc(M[:, ID[0][j]], 25)
            InterQuart = Quart75 - Quart25
            MaxBound = Quart75 + NInterQuart * InterQuart
            MinBound = Quart25 - NInterQuart * InterQuart
            if np.where((M[:, ID[0][j]] &lt; MinBound) | (M[:, ID[0][j]] &gt; MaxBound))[0].shape[0] == 1:
                NMFPriors[ID[0][j], k] = 0

        ID = np.where(NMFPriors[:, k] &gt; 0)
        pk = ID[0].shape[0]
        if pk == 0:
            ErrMessage = &#39;Error! Null column in NMF priors (&#39; + str(k+1) + &#39;, post outlier filtering)&#39;
            return [NMFPriors, AddMessage, ErrMessage]

        # Characterize clusters by skewness direction
        Mtc = Mt[:, k] - np.mean(Mt[:, k])
        std = math.sqrt(np.mean(Mtc ** 2))
        skewness = np.mean((Mtc / std) ** 3) * math.sqrt(n * (n - 1)) / (n - 2)

        # Scale columns and initialized weights
        for j in range(0, pk):
            M[:, ID[0][j]] /= np.sum(M[:, ID[0][j]])
            Mc[:, ID[0][j]] = M[:, ID[0][j]] - np.mean(M[:, ID[0][j]])
            std = math.sqrt(np.mean(Mc[:, ID[0][j]] ** 2))
            Mweight[ID[0][j], k] = np.mean((Mc[:, ID[0][j]] / std) ** 3) * math.sqrt(n * (n - 1)) / (n - 2)

        if skewness &lt; 0:
            # Negative skewness =&gt; Component identifiable through small proportions
            Mweight[Mweight[:, k] &gt; 0, k] = 0
            Mweight = -Mweight
            IDneg = np.where(Mweight[:, k] &gt; 0)
            Nneg = IDneg[0].shape[0]
            if Nneg == 0:
                ErrMessage = &#39;Error! No marker variable found in component &#39; + str(k+1)
                return [NMFPriors, AddMessage, ErrMessage]

            AddMessage.insert(len(AddMessage),
                              &#39;Component &#39; + str(k+1) + &#39;: compositions are negatively skewed (&#39; + str(
                                  Nneg) + &#39; active variables)&#39;)
        else:
            # Positive skewness =&gt; Component identifiable through large proportions
            Mweight[Mweight[:, k] &lt; 0, k] = 0
            IDpos = np.where(Mweight[:, k] &gt; 0)
            Npos = IDpos[0].shape[0]
            if Npos == 0:
                ErrMessage = &#39;Error! No marker variable found in component &#39; + str(k+1)
                return [NMFPriors, AddMessage, ErrMessage]

            AddMessage.insert(len(AddMessage),
                              &#39;Component &#39; + str(k+1) + &#39;: compositions are positively skewed (&#39; + str(
                                  Npos) + &#39; active variables)&#39;)

        # Logistic transform of non-zero weights
        ID2 = np.where(Mweight[:, k] &gt; 0)
        n_ID2 = ID2[0].shape[0]
        if n_ID2 &gt; 1:
            mu = np.mean(Mweight[ID2[0], k])
            std = np.std(Mweight[ID2[0], k])
            Mweight[ID2[0], k] = (Mweight[ID2[0], k] - mu) / std
            Mweight[ID2[0], k] = np.ones(n_ID2) - np.ones(n_ID2) / (np.ones(n_ID2) + np.exp(
                2 * (Mweight[ID2[0], k] - percentile_exc(Mweight[ID2[0], k], 90))))
        else:
            Mweight[ID2[0], k] = 1

        # ReWeigh columns
        M[:, ID[0]] = M[:, ID[0]] * Mweight[ID[0], k].T

        # Update NMF priors (cancel columns with 0 weight &amp; replace non zero values by 1)
        NMFPriors[ID[0], k] = NMFPriors[ID[0], k] * Mweight[ID[0], k]
        ID = np.where(NMFPriors[:, k] &gt; 0)
        if ID[0].shape[0] &gt; 0:
            NMFPriors[ID[0], k] = 1
            # Scale parts
            M[:, ID[0]] /= np.linalg.norm(M[:, ID[0]])
        else:
            ErrMessage = &#39;Error! Null column in NMF priors (&#39; + str(k+1) + &#39;, post cancelling 0-weight columns)&#39;
            return [NMFPriors, AddMessage, ErrMessage]

    return [NMFPriors, AddMessage, ErrMessage]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NMFSolve"><code class="name flex">
<span>def <span class="ident">NMFSolve</span></span>(<span>M, Mmis, Mt0, Mw0, nc, tolerance, precision, LogIter, Status0, MaxIterations, NMFAlgo, NMFFixUserLHE, NMFFixUserRHE, NMFMaxInterm, NMFMaxIterProj, NMFSparseLevel, NMFFindParts, NMFFindCentroids, NMFKernel, NMFReweighColumns, NMFPriors, flagNonconvex, AddMessage, myStatusBox)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate left and right hand matrices</p>
<h2 id="input">Input</h2>
<p>M: Input matrix
Mmis: Define missing values (0 = missing cell, 1 = real cell)
Mt0: Initial left hand matrix
Mw0: Initial right hand matrix
nc: NMF rank
tolerance: Convergence threshold
precision: Replace 0-value in multiplication rules
LogIter: Log results through iterations
Status0: Initial displayed status to be updated during iterations
MaxIterations: Max iterations
NMFAlgo: =1,3: Divergence; =2,4: Least squares;
NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
NMFFixUserRHE: = 1 =&gt; fixed
right hand matrix columns
NMFMaxInterm: Max iterations for warmup multiplication rules
NMFMaxIterProj: Max iterations for projected gradient
NMFSparseLevel: Requested sparsity in terms of relative number of rows with 0 values in right hand matrix
NMFFindParts: Enforce convexity on left hand matrix
NMFFindCentroids: Enforce convexity on right hand matrix
NMFKernel: Type of kernel used; 1: linear; 2: quadratic; 3: radial
NMFReweighColumns: Reweigh columns in 2nd step of parts-based NMF
NMFPriors: Priors on right hand matrix
flagNonconvex: Non-convexity flag on left hand matrix</p>
<h2 id="output">Output</h2>
<p>Mt: Left hand matrix
Mw: Right hand matrix
diff: objective cost
Mh: Convexity matrix
NMFPriors: Updated priors on right hand matrix
flagNonconvex: Updated non-convexity flag on left hand matrix</p>
<h2 id="reference">Reference</h2>
<p>C. H.Q. Ding et al (2010) Convex and Semi-Nonnegative Matrix Factorizations
IEEE Transactions on Pattern Analysis and Machine Intelligence Vol: 32 Issue: 1</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NMFSolve(M, Mmis, Mt0, Mw0, nc, tolerance, precision, LogIter, Status0, MaxIterations, NMFAlgo,
             NMFFixUserLHE, NMFFixUserRHE, NMFMaxInterm, NMFMaxIterProj, NMFSparseLevel,
             NMFFindParts, NMFFindCentroids, NMFKernel, NMFReweighColumns, NMFPriors, flagNonconvex, AddMessage,
             myStatusBox):
    &#34;&#34;&#34;
    Estimate left and right hand matrices
    Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         nc: NMF rank
         tolerance: Convergence threshold
         precision: Replace 0-value in multiplication rules
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFAlgo: =1,3: Divergence; =2,4: Least squares;
         NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
         NMFFixUserRHE: = 1 =&gt; fixed  right hand matrix columns
         NMFMaxInterm: Max iterations for warmup multiplication rules
         NMFMaxIterProj: Max iterations for projected gradient
         NMFSparseLevel: Requested sparsity in terms of relative number of rows with 0 values in right hand matrix
         NMFFindParts: Enforce convexity on left hand matrix
         NMFFindCentroids: Enforce convexity on right hand matrix
         NMFKernel: Type of kernel used; 1: linear; 2: quadratic; 3: radial
         NMFReweighColumns: Reweigh columns in 2nd step of parts-based NMF
         NMFPriors: Priors on right hand matrix
         flagNonconvex: Non-convexity flag on left hand matrix
    Output:
         Mt: Left hand matrix
         Mw: Right hand matrix
         diff: objective cost
         Mh: Convexity matrix
         NMFPriors: Updated priors on right hand matrix
         flagNonconvex: Updated non-convexity flag on left hand matrix
    
    Reference
    ---------

    C. H.Q. Ding et al (2010) Convex and Semi-Nonnegative Matrix Factorizations
    IEEE Transactions on Pattern Analysis and Machine Intelligence Vol: 32 Issue: 1

    &#34;&#34;&#34;
    ErrMessage = &#39;&#39;
    cancel_pressed = 0

    n, p = M.shape
    n_Mmis = Mmis.shape[0]
    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    nc = int(nc)
    nxp = int(n * p)
    Mh = np.array([])
    Mt = np.copy(Mt0)
    Mw = np.copy(Mw0)
    diff = 1.e+99

    # Add weights
    if n_NMFPriors &gt; 0:
        if NMFReweighColumns &gt; 0:
            # A local copy of M will be updated
            M = np.copy(M)
            NMFPriors, AddMessage, ErrMessage = NMFReweigh(M, Mt, NMFPriors, AddMessage)
            if ErrMessage != &#34;&#34;:
                return [Mt, Mw, diff, Mh, NMFPriors, flagNonconvex, AddMessage, ErrMessage, cancel_pressed]
        else:
            NMFPriors[NMFPriors &gt; 0] = 1

    if (NMFFindParts &gt; 0) &amp; (NMFFixUserLHE &gt; 0):
        NMFFindParts = 0

    if (NMFFindCentroids &gt; 0) &amp; (NMFFixUserRHE &gt; 0):
        NMFFindCentroids = 0
        NMFKernel = 1

    if (NMFFindCentroids &gt; 0) &amp; (NMFKernel &gt; 1):
        if n_Mmis &gt; 0:
            NMFKernel = 1
            AddMessage.insert(len(AddMessage), &#39;Warning: Non linear kernel canceled due to missing values.&#39;)

        if (NMFAlgo == 1) or (NMFAlgo == 3) :
            NMFKernel = 1
            AddMessage.insert(len(AddMessage), &#39;Warning: Non linear kernel canceled due to divergence minimization.&#39;)

    if n_NMFPriors &gt; 0:
        MwUser = NMFPriors
        for k in range(0, nc):
            if (NMFAlgo == 2) | (NMFAlgo == 4):
                Mw[:, k] = MwUser[:, k] / np.linalg.norm(MwUser[:, k])
            else:
                Mw[:, k] = MwUser[:, k] / np.sum(MwUser[:, k])

    MultOrPgrad = 1  # Start with Lee-Seung mult rules
    MaxIterations += NMFMaxInterm  # NMFMaxInterm Li-Seung iterations initialize projected gradient

    StepIter = math.ceil(MaxIterations / 10)
    pbar_step = 100 * StepIter / MaxIterations

    iIter = 0
    cont = 1

    # Initialize penalty
    # lambda = -1: no penalty
    # lambda = -abs(NMFSparselevel) : initialisation by NMFSparselevel (in negative value)
    if NMFSparseLevel &gt; 0:
        lambdaw = -NMFSparseLevel
        lambdat = -1
    elif NMFSparseLevel &lt; 0:
        lambdat = NMFSparseLevel
        lambdaw = -1
    else:
        lambdaw = -1
        lambdat = -1

    PercentZeros = 0
    iterSparse = 0
    NMFConvex = 0
    NLKernelApplied = 0

    myStatusBox.init_bar(delay=1)
    
    # Start loop
    while (cont == 1) &amp; (iIter &lt; MaxIterations):
        # Update RHE
        if NMFFixUserRHE == 0:
            if MultOrPgrad == 1:
                if (NMFAlgo == 2) or (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Mw = \
                            Mw * ((Mt.T @ (M * Mmis)) / (
                                    Mt.T @ ((Mt @ Mw.T) * Mmis) + precision)).T
                    else:
                        Mw = \
                             Mw * ((Mt.T @ M) / (
                                (Mt.T @ Mt) @ Mw.T + precision)).T
                else:
                    if n_Mmis &gt; 0:
                        Mw = Mw * (((M * Mmis) / ((Mt @ Mw.T) * Mmis + precision)).T @ Mt)
                    else:
                        Mw = Mw * ((M / (Mt @ Mw.T + precision)).T @ Mt)

                if n_NMFPriors &gt; 0:
                    Mw = Mw * NMFPriors
            else:
                # Projected gradient
                if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                    Mw, tolMw = NMFProjGradKernel(Kernel, M, Mmis, Mh, Mw, NMFAlgo, tolMw, NMFMaxIterProj, NMFPriors.T)
                elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0):
                    Mh, tolMh, dummy = NMFProjGrad(In, np.array([]), Mt, Mh.T, NMFAlgo, -1, tolMh, NMFMaxIterProj, np.array([]))
                else:
                    Mw, tolMw, lambdaw = NMFProjGrad(M, Mmis, Mt, Mw.T, NMFAlgo, lambdaw, tolMw, \
                                                                NMFMaxIterProj, NMFPriors.T)

            if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                for k in range(0, nc):
                    ScaleMw = np.linalg.norm(Mw[:, k])
                    Mw[:, k] = Mw[:, k] / ScaleMw
                    Mt[:, k] = Mt[:, k] * ScaleMw

        # Update LHE
        if NMFFixUserLHE == 0:
            if MultOrPgrad == 1:
                if (NMFAlgo == 2) | (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Mt = \
                            Mt * ((M * Mmis) @ Mw / (
                                ((Mt @ Mw.T) * Mmis) @ Mw + precision))
                    else:
                        Mt = \
                            Mt * (M @ Mw / (Mt @ (Mw.T @ Mw) + precision))
                else:
                    if n_Mmis &gt; 0:
                        Mt = Mt * (((M * Mmis).T / (Mw @ Mt.T + precision)).T @ Mw)
                    else:
                        Mt = Mt * ((M.T / (Mw @ Mt.T + precision)).T @ Mw)
            else:
                # Projected gradient
                if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                    Mh, tolMh, dummy = NMFProjGrad(Ip, np.array([]), Mw, Mh.T, NMFAlgo, -1, tolMh, NMFMaxIterProj, np.array([]))
                elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0):
                    Mt, tolMt = NMFProjGradKernel(Kernel, M.T, Mmis.T, Mh, Mt, NMFAlgo, tolMt, NMFMaxIterProj, np.array([]))
                else:
                    Mt, tolMt, lambdat = NMFProjGrad(M.T, Mmis.T, Mw, Mt.T, NMFAlgo,
                                                            lambdat, tolMt, NMFMaxIterProj, np.array([]))

            # Scaling
            if ((NMFConvex == 0) | (NMFFindCentroids &gt; 0)) &amp; (NMFFixUserLHE == 0) &amp;  (NMFFixUserRHE == 0):
                for k in range(0, nc):
                    if (NMFAlgo == 2) | (NMFAlgo == 4):
                        ScaleMt = np.linalg.norm(Mt[:, k])
                    else:
                        ScaleMt = np.sum(Mt[:, k])

                    if ScaleMt &gt; 0:
                        Mt[:, k] = Mt[:, k] / ScaleMt
                        if MultOrPgrad == 2:
                            Mw[:, k] = Mw[:, k] * ScaleMt

        # Switch to projected gradient
        if iIter == NMFMaxInterm:
            MultOrPgrad = 2
            StepIter = 1
            pbar_step = 100 / MaxIterations
            gradMt = Mt @ (Mw.T @ Mw) - M @ Mw
            gradMw = ((Mt.T @ Mt) @ Mw.T - Mt.T @ M).T
            initgrad = np.linalg.norm(np.concatenate((gradMt, gradMw), axis=0))
            tolMt = 1e-3 * initgrad
            tolMw = tolMt

        if iIter % StepIter == 0:
            if (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                MhtKernel = Mh.T @ Kernel
                diff = (TraceKernel + np.trace(-2 * Mw @ MhtKernel + Mw @ (MhtKernel @ Mh) @ Mw.T)) / nxp
            elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0):
                MhtKernel = Mh.T @ Kernel
                diff = (TraceKernel + np.trace(-2 * Mt @ MhtKernel + Mt @ (MhtKernel @ Mh) @ Mt.T)) / nxp
            else:
                if (NMFAlgo == 2) | (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Mdiff = (Mt @ Mw.T - M) * Mmis
                    else:
                        Mdiff = Mt @ Mw.T - M

                else:
                    MF0 = Mt @ Mw.T
                    Mdiff = M * np.log(M / MF0 + precision) + MF0 - M

                diff = (np.linalg.norm(Mdiff)) ** 2 / nxp

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)

            if NMFSparseLevel != 0:
                if NMFSparseLevel &gt; 0:
                    lambdax = lambdaw
                else:
                    lambdax = lambdat

                Status = Status + &#39;; Achieved sparsity: &#39; + str(round(PercentZeros, 2)) + &#39;; Penalty: &#39; + str(
                    round(lambdax, 2))
                if LogIter == 1:
                    myStatusBox.myPrint(Status) 
            elif (NMFConvex &gt; 0) &amp; (NMFFindParts &gt; 0):
                Status = Status + &#39; - Find parts&#39;
            elif (NMFConvex &gt; 0) &amp; (NMFFindCentroids &gt; 0) &amp; (NLKernelApplied == 0):
                Status = Status + &#39; - Find centroids&#39;
            elif NLKernelApplied == 1:
                Status = Status + &#39; - Apply non linear kernel&#39;

            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [Mt, Mw, diff, Mh, NMFPriors, flagNonconvex, AddMessage, ErrMessage, cancel_pressed]

            if LogIter == 1:
                if (NMFAlgo == 2) | (NMFAlgo == 4):
                    myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))
                else:
                    myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; DIV: &#34; + str(diff))

            if iIter &gt; NMFMaxInterm:
                if (diff0 - diff) / diff0 &lt; tolerance:
                    cont = 0

            diff0 = diff

        iIter += 1

        if (cont == 0) | (iIter == MaxIterations):
            if ((NMFFindParts &gt; 0) | (NMFFindCentroids &gt; 0)) &amp; (NMFConvex == 0):
                # Initialize convexity
                NMFConvex = 1
                diff0 = 1.e+99
                iIter = NMFMaxInterm + 1
                myStatusBox.init_bar(delay=1)
                cont = 1
                if NMFFindParts &gt; 0:
                    Ip = np.identity(p)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis * M, 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M, 1, np.array([]), np.array([]))
                    else:
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis * (M / (Mt @ Mw.T)), 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M / (Mt @ Mw.T), 1, np.array([]), np.array([]))

                    TraceKernel = np.trace(Kernel)
                    try:
                        Mh = Mw @ np.linalg.inv(Mw.T @ Mw)
                    except:
                        Mh = Mw @ np.linalg.pinv(Mw.T @ Mw)

                    Mh[np.where(Mh &lt; 0)] = 0
                    for k in range(0, nc):
                        ScaleMw = np.linalg.norm(Mw[:, k])
                        Mw[:, k] = Mw[:, k] / ScaleMw
                        Mh[:, k] = Mh[:, k] * ScaleMw

                    gradMh = Mh @ (Mw.T @ Mw) - Mw
                    gradMw = ((Mh.T @ Mh) @ Mw.T - Mh.T).T
                    initgrad = np.linalg.norm(np.concatenate((gradMh, gradMw), axis=0))
                    tolMh = 1.e-3 * initgrad
                    tolMw = tolMt
                elif NMFFindCentroids &gt; 0:
                    In = np.identity(n)
                    if (NMFAlgo == 2) or (NMFAlgo == 4):
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis.T * M.T, 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M.T, 1, np.array([]), np.array([]))
                    else:
                        if n_Mmis &gt; 0:
                            Kernel = NMFApplyKernel(Mmis.T * (M.T / (Mt @ Mw.T).T), 1, np.array([]), np.array([]))
                        else:
                            Kernel = NMFApplyKernel(M.T / (Mt @ Mw.T).T, 1, np.array([]), np.array([]))

                    TraceKernel = np.trace(Kernel)
                    try:
                        Mh = Mt @ np.linalg.inv(Mt.T @ Mt)
                    except:
                        Mh = Mt @ np.linalg.pinv(Mt.T @ Mt)

                    Mh[np.where(Mh &lt; 0)] = 0
                    for k in range(0, nc):
                        ScaleMt = np.linalg.norm(Mt[:, k])
                        Mt[:, k] = Mt[:, k] / ScaleMt
                        Mh[:, k] = Mh[:, k] * ScaleMt

                    gradMt = Mt @ (Mh.T @ Mh) - Mh
                    gradMh = ((Mt.T @ Mt) @ Mh.T - Mt.T).T
                    initgrad = np.linalg.norm(np.concatenate((gradMt, gradMh), axis=0))
                    tolMh = 1.e-3 * initgrad
                    tolMt = tolMh

            elif (NMFConvex &gt; 0) &amp; (NMFKernel &gt; 1) &amp; (NLKernelApplied == 0):
                NLKernelApplied = 1
                diff0 = 1.e+99
                iIter = NMFMaxInterm + 1
                myStatusBox.init_bar(delay=1)
                cont = 1
                # Calculate centroids
                for k in range(0, nc):
                    Mh[:, k] = Mh[:, k] / np.sum(Mh[:, k])

                Mw = (Mh.T @ M).T
                if (NMFAlgo == 2) or (NMFAlgo == 4):
                    if n_Mmis &gt; 0:
                        Kernel = NMFApplyKernel(Mmis.T * M.T, NMFKernel, Mw, Mt)
                    else:
                        Kernel = NMFApplyKernel(M.T, NMFKernel, Mw, Mt)
                else:
                    if n_Mmis &gt; 0:
                        Kernel = NMFApplyKernel(Mmis.T * (M.T / (Mt @ Mw.T).T), NMFKernel, Mw, Mt)
                    else:
                        Kernel = NMFApplyKernel(M.T / (Mt @ Mw.T).T, NMFKernel, Mw, Mt)

                TraceKernel = np.trace(Kernel)
                try:
                    Mh = Mt @ np.linalg.inv(Mt.T @ Mt)
                except:
                    Mh = Mt @ np.linalg.pinv(Mt.T @ Mt)

                Mh[np.where(Mh &lt; 0)] = 0
                for k in range(0, nc):
                    ScaleMt = np.linalg.norm(Mt[:, k])
                    Mt[:, k] = Mt[:, k] / ScaleMt
                    Mh[:, k] = Mh[:, k] * ScaleMt

                gradMt = Mt @ (Mh.T @ Mh) - Mh
                gradMh = ((Mt.T @ Mt) @ Mh.T - Mt.T).T
                initgrad = np.linalg.norm(np.concatenate((gradMt, gradMh), axis=0))
                tolMh = 1.e-3 * initgrad
                tolMt = tolMh

            if NMFSparseLevel &gt; 0:
                SparseTest = np.zeros((p, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mw[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / p, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
                    lambdaw *= min(1.01 * NMFSparseLevel / PercentZeros, 1.10)
                    iIter = NMFMaxInterm + 1
                    cont = 1

            elif NMFSparseLevel &lt; 0:
                SparseTest = np.zeros((n, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mt[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / n, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
                    lambdat *= min(1.01 * abs(NMFSparseLevel) / PercentZeros, 1.10)
                    iIter = NMFMaxInterm + 1
                    cont = 1

    if NMFFindParts &gt; 0:
        # Make Mt convex
        Mt = M @ Mh
        Mt, Mw, Mh, flagNonconvex, AddMessage, ErrMessage, cancel_pressed = NMFGetConvexScores(Mt, Mw, Mh, flagNonconvex,
                                                                                         AddMessage)
    elif NMFFindCentroids &gt; 0:
        # Calculate row centroids
        for k in range(0, nc):
            ScaleMh = np.sum(Mh[:, k])
            Mh[:, k] = Mh[:, k] / ScaleMh
            Mt[:, k] = Mt[:, k] * ScaleMh

        Mw = (Mh.T @ M).T

    if (NMFKernel &gt; 1) &amp; (NLKernelApplied == 1):
        diff /= TraceKernel / nxp

    return [Mt, Mw, diff, Mh, NMFPriors, flagNonconvex, AddMessage, ErrMessage, cancel_pressed]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NTFSolve"><code class="name flex">
<span>def <span class="ident">NTFSolve</span></span>(<span>M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE, NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface to:
- NTFSolve_simple
- NTFSolve_conv</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NTFSolve(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox):
    &#34;&#34;&#34;Interface to:
            - NTFSolve_simple
            - NTFSolve_conv
    &#34;&#34;&#34;

    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    if n_NMFPriors &gt; 0:
        NMFPriors[NMFPriors &gt; 0] = 1

    if NTFNConv &gt; 0:
        return NTFSolve_conv(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox)
    else:
        return NTFSolve_simple(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NMFPriors, myStatusBox)</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NTFSolveFast"><code class="name flex">
<span>def <span class="ident">NTFSolveFast</span></span>(<span>M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, precision, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, myStatusBox)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate NTF matrices (fast HALS)</p>
<h2 id="input">Input</h2>
<p>M: Input matrix
Mmis: Define missing values (0 = missing cell, 1 = real cell)
Mt0: Initial left hand matrix
Mw0: Initial right hand matrix
Mb0: Initial block hand matrix
nc: NTF rank
tolerance: Convergence threshold
precision: Replace 0-values in multiplication rules
LogIter: Log results through iterations
Status0: Initial displayed status to be updated during iterations
MaxIterations: Max iterations
NMFFixUserLHE: fix left hand matrix columns: = 1, else = 0
NMFFixUserRHE: fix
right hand matrix columns: = 1, else = 0
NMFFixUserBHE: fix
block hand matrix columns: = 1, else = 0
NTFUnimodal: Apply Unimodal constraint on factoring vectors
NTFSmooth: Apply Smooth constraint on factoring vectors
NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
NBlocks: Number of NTF blocks</p>
<h2 id="output">Output</h2>
<p>Mt: Left hand matrix
Mw: Right hand matrix
Mb: Block hand matrix
diff: objective cost</p>
<p>Note: This code does not support missing values, nor sparsity constraint</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NTFSolveFast(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, precision, LogIter, Status0, MaxIterations, NMFFixUserLHE,
                 NMFFixUserRHE, NMFFixUserBHE, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents,
                 NBlocks, myStatusBox):
    &#34;&#34;&#34;Estimate NTF matrices (fast HALS)
     Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         Mb0: Initial block hand matrix
         nc: NTF rank
         tolerance: Convergence threshold
         precision: Replace 0-values in multiplication rules
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFFixUserLHE: fix left hand matrix columns: = 1, else = 0
         NMFFixUserRHE: fix  right hand matrix columns: = 1, else = 0
         NMFFixUserBHE: fix  block hand matrix columns: = 1, else = 0
         NTFUnimodal: Apply Unimodal constraint on factoring vectors
         NTFSmooth: Apply Smooth constraint on factoring vectors
         NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
         NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
         NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
         NBlocks: Number of NTF blocks
     Output:
         Mt: Left hand matrix
         Mw: Right hand matrix
         Mb: Block hand matrix
         diff: objective cost

     Note: This code does not support missing values, nor sparsity constraint

     &#34;&#34;&#34;
    Mres = np.array([])
    cancel_pressed = 0

    n, p0 = M.shape
    n_Mmis = Mmis.shape[0]
    nc = int(nc)
    NBlocks = int(NBlocks)
    p = int(p0 / NBlocks)
    n0 = int(n * NBlocks)
    nxp = int(n * p)
    nxp0 = int(n * p0)
    Mt = np.copy(Mt0)
    Mw = np.copy(Mw0)
    Mb = np.copy(Mb0)
    StepIter = math.ceil(MaxIterations / 10)
    pbar_step = 100 * StepIter / MaxIterations

    IDBlockn = np.arange(0, (NBlocks - 1) * n + 1, n)
    IDBlockp = np.arange(0, (NBlocks - 1) * p + 1, p)
    A = np.zeros(n)
    B = np.zeros(p)
    C = np.zeros(NBlocks)

    if NMFFixUserBHE &gt; 0:
        NormBHE = True
        if NMFFixUserRHE == 0:
            NormLHE = True
            NormRHE = False
        else:
            NormLHE = False
            NormRHE = True
    else:
            NormBHE = False
            NormLHE = True
            NormRHE = True

    for k in range(0, nc):
        if (NMFFixUserLHE &gt; 0) &amp; NormLHE:
            norm = np.linalg.norm(Mt[:, k])
            if norm &gt; 0:
                Mt[:, k] /= norm
    
        if (NMFFixUserRHE &gt; 0) &amp; NormRHE:
            norm = np.linalg.norm(Mw[:, k])
            if norm &gt; 0:
                Mw[:, k] /= norm
            
        if (NMFFixUserBHE &gt; 0) &amp; NormBHE:
            norm = np.linalg.norm(Mb[:, k])
            if norm &gt; 0:
                Mb[:, k] /= norm
    
    # Normalize factors to unit length
    #    for k in range(0, nc):
    #        ScaleMt = np.linalg.norm(Mt[:, k])
    #        Mt[:, k] /= ScaleMt
    #        ScaleMw = np.linalg.norm(Mw[:, k])
    #        Mw[:, k] /= ScaleMw
    #        Mb[:, k] *= (ScaleMt * ScaleMw)

    # Initialize T1
    Mt2 = Mt.T @ Mt
    Mt2[Mt2 == 0] = precision
    Mw2 = Mw.T @ Mw
    Mw2[Mw2 == 0] = precision
    Mb2 = Mb.T @ Mb
    Mb2[Mb2 == 0] = precision
    T1 = Mt2 * Mw2 * Mb2
    T2t = np.zeros((n, nc))
    T2w = np.zeros((p, nc))
    T2Block = np.zeros((NBlocks, nc))

    # Transpose M by block once for all
    M2 = np.zeros((p, n0))

    Mfit = np.zeros((n, p0))
    if n_Mmis &gt; 0:
        denomt = np.zeros(n)
        denomw = np.zeros(p)
        denomBlock = np.ones((NBlocks, nc))
        MxMmis2 = np.zeros((p, n0))
        denomCutoff = .1

    myStatusBox.init_bar(delay=1)

    # Loop
    cont = 1
    iIter = 0
    diff0 = 1.e+99


    for iBlock in range(0, NBlocks):
        M2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] = M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T
        if n_Mmis &gt; 0:
            MxMmis2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] = (M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] * \
                                                                 Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p]).T

    if n_Mmis &gt; 0:
        MxMmis = M * Mmis

    while (cont &gt; 0) &amp; (iIter &lt; MaxIterations):
        if n_Mmis &gt; 0:
            Gamma = np.diag((denomBlock*Mb).T @ (denomBlock*Mb))
        else:
            Gamma = np.diag(Mb.T @ Mb)

        if NMFFixUserLHE == 0:
            # Update Mt
            T2t[:,:] = 0
            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    denomt[:] = 0
                    Mwn = np.repeat(Mw[:, k, np.newaxis] ** 2, n, axis=1)
                    for iBlock in range(0, NBlocks):
                        # Broadcast missing cells into Mw to calculate Mw.T * Mw
                        denomt += Mb[iBlock, k]**2 * np.sum(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T * Mwn, axis = 0)

                    denomt /= np.max(denomt)
                    denomt[denomt &lt; denomCutoff] = denomCutoff
                    for iBlock in range(0, NBlocks):
                        T2t[:, k] += MxMmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw[:, k] * Mb[iBlock, k]

                    T2t[:, k] /= denomt
                else:
                    for iBlock in range(0, NBlocks):
                        T2t[:, k] += M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw[:, k] * Mb[iBlock, k]

            Mt2 = Mt.T @ Mt
            Mt2[Mt2 == 0] = precision
            T3 = T1 / Mt2

            for k in range(0, nc):
                Mt[:, k] = Gamma[k] * Mt[:, k] + T2t[:, k] - Mt @ T3[:, k]
                Mt[np.where(Mt[:, k] &lt; 0), k] = 0

                if (NTFUnimodal &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #                 Enforce unimodal distribution
                    tmax = np.argmax(Mt[:, k])
                    for i in range(tmax + 1, n):
                        Mt[i, k] = min(Mt[i - 1, k], Mt[i, k])

                    for i in range(tmax - 1, -1, -1):
                        Mt[i, k] = min(Mt[i + 1, k], Mt[i, k])

                if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #             Smooth distribution
                    A[0] = .75 * Mt[0, k] + .25 * Mt[1, k]
                    A[n - 1] = .25 * Mt[n - 2, k] + .75 * Mt[n - 1, k]
                    for i in range(1, n - 1):
                        A[i] = .25 * Mt[i - 1, k] + .5 * Mt[i, k] + .25 * Mt[i + 1, k]

                    Mt[:, k] = A

                if NormLHE:
                    Mt[:, k] /= np.linalg.norm(Mt[:, k])

            Mt2 = Mt.T @ Mt
            Mt2[Mt2 == 0] = precision
            T1 = T3 * Mt2

        if NMFFixUserRHE == 0:
            # Update Mw
            T2w[:,:] = 0
            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    denomw[:] = 0
                    Mtp = np.repeat(Mt[:, k, np.newaxis] ** 2, p, axis=1)
                    for iBlock in range(0, NBlocks):
                        # Broadcast missing cells into Mw to calculate Mt.T * Mt
                        denomw += Mb[iBlock, k]**2 * np.sum(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] * Mtp, axis = 0)

                    denomw /= np.max(denomw)
                    denomw[denomw &lt; denomCutoff] = denomCutoff
                    for iBlock in range(0, NBlocks):
                        T2w[:, k] += MxMmis2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] @ Mt[:, k] * Mb[iBlock, k]

                    T2w[:, k] /= denomw
                else:
                    for iBlock in range(0, NBlocks):
                        T2w[:, k] += M2[:, IDBlockn[iBlock]:IDBlockn[iBlock] + n] @ Mt[:, k] * Mb[iBlock, k]

            Mw2 = Mw.T @ Mw
            Mw2[Mw2 == 0] = precision
            T3 = T1 / Mw2

            for k in range(0, nc):
                Mw[:, k] = Gamma[k] * Mw[:, k] + T2w[:, k] - Mw @ T3[:, k]
                Mw[np.where(Mw[:, k] &lt; 0), k] = 0

                if (NTFUnimodal &gt; 0) &amp; (NTFRightComponents &gt; 0):
                    #                 Enforce unimodal distribution
                    wmax = np.argmax(Mw[:, k])
                    for j in range(wmax + 1, p):
                        Mw[j, k] = min(Mw[j - 1, k], Mw[j, k])

                    for j in range(wmax - 1, -1, -1):
                        Mw[j, k] = min(Mw[j + 1, k], Mw[j, k])

                if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #             Smooth distribution
                    B[0] = .75 * Mw[0, k] + .25 * Mw[1, k]
                    B[p - 1] = .25 * Mw[p - 2, k] + .75 * Mw[p - 1, k]
                    for j in range(1, p - 1):
                        B[j] = .25 * Mw[j - 1, k] + .5 * Mw[j, k] + .25 * Mw[j + 1, k]

                    Mw[:, k] = B

                if NormRHE:
                    Mw[:, k] /= np.linalg.norm(Mw[:, k])

            Mw2 = Mw.T @ Mw
            Mw2[Mw2 == 0] = precision
            T1 = T3 * Mw2

        if NMFFixUserBHE == 0:
            # Update Mb
            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    for iBlock in range(0, NBlocks):
                        # Broadcast missing cells into Mb to calculate Mb.T * Mb
                        denomBlock[iBlock, k] = np.sum(np.reshape(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp) *
                                np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp)**2, axis=0)

                    maxdenomBlock = np.max(denomBlock[:, k])
                    denomBlock[denomBlock[:, k] &lt; denomCutoff * maxdenomBlock] = denomCutoff * maxdenomBlock
                    for iBlock in range(0, NBlocks):
                        T2Block[iBlock, k] = np.reshape(MxMmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp).T @ \
                                        (np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp)) / denomBlock[iBlock, k]

                else:
                    for iBlock in range(0, NBlocks):
                        T2Block[iBlock, k] = np.reshape(M[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp).T @ \
                                        (np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp))

            Mb2 = Mb.T @ Mb
            Mb2[Mb2 == 0] = precision
            T3 = T1 / Mb2

            for k in range(0, nc):
                Mb[:, k] = Mb[:, k] + T2Block[:, k] - Mb @ T3[:, k]
                Mb[np.where(Mb[:, k] &lt; 0), k] = 0

                if (NTFUnimodal &gt; 0) &amp; (NTFBlockComponents &gt; 0):
                    #                 Enforce unimodal distribution
                    bmax = np.argmax(Mb[:, k])
                    for iBlock in range(bmax + 1, NBlocks):
                        Mb[iBlock, k] = min(Mb[iBlock - 1, k], Mb[iBlock, k])

                    for iBlock in range(bmax - 1, -1, -1):
                        Mb[iBlock, k] = min(Mb[iBlock + 1, k], Mb[iBlock, k])

                if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
                    #             Smooth distribution
                    C[0] = .75 * Mb[0, k] + .25 * Mb[1, k]
                    C[NBlocks - 1] = .25 * Mb[NBlocks - 2, k] + .75 * Mb[NBlocks - 1, k]
                    for iBlock in range(1, NBlocks - 1):
                        C[iBlock] = .25 * Mb[iBlock - 1, k] + .5 * Mb[iBlock, k] + .25 * Mb[iBlock + 1, k]

                    Mb[:, k] = C

            Mb2 = Mb.T @ Mb
            Mb2[Mb2 == 0] = precision
            T1 = T3 * Mb2

        if iIter % StepIter == 0:
            # Update residual tensor
            Mfit[:,:] = 0

            for k in range(0, nc):
                if n_Mmis &gt; 0:
                    for iBlock in range(0, NBlocks):
                        #Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += denomBlock[iBlock, k] * Mb[iBlock, k] * (
                        Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * (
                        np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p)))

                    Mres = (M - Mfit) * Mmis
                else:
                    for iBlock in range(0, NBlocks):
                        Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * (
                                np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p)))

                    Mres = (M - Mfit)

            # Check convergence
            diff = np.linalg.norm(Mres) ** 2 / nxp0
            if (diff0 - diff) / diff0 &lt; tolerance:
                cont = 0
            else:
                diff0 = diff

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)
            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [Mt, Mw, Mb, Mres, cancel_pressed]

            if LogIter == 1:
                myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))

        iIter += 1

    if n_Mmis &gt; 0:
        Mb *= denomBlock

    return [Mt, Mw, Mb, diff, cancel_pressed]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NTFSolve_conv"><code class="name flex">
<span>def <span class="ident">NTFSolve_conv</span></span>(<span>M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE, NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate NTF matrices (HALS)</p>
<h2 id="input">Input</h2>
<p>M: Input matrix
Mmis: Define missing values (0 = missing cell, 1 = real cell)
Mt0: Initial left hand matrix
Mw0: Initial right hand matrix
Mb0: Initial block hand matrix
nc: NTF rank
tolerance: Convergence threshold
LogIter: Log results through iterations
Status0: Initial displayed status to be updated during iterations
MaxIterations: Max iterations
NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
NMFFixUserRHE: = 1 =&gt; fixed
right hand matrix columns
NMFFixUserBHE: = 1 =&gt; fixed
block hand matrix columns
NMFSparseLevel : sparsity level (as defined by Hoyer); +/- = make RHE/LHe sparse
NTFUnimodal: Apply Unimodal constraint on factoring vectors
NTFSmooth: Apply Smooth constraint on factoring vectors
NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
NBlocks: Number of NTF blocks
NTFNConv: Half-Size of the convolution window on 3rd-dimension of the tensor
NMFPriors: Elements in Mw that should be updated (others remain 0)</p>
<h2 id="output">Output</h2>
<p>Mt : if NTFNConv &gt; 0 only otherwise empty. Contains sub-components for each phase in convolution window
Mt_simple: Left hand matrix (sum of columns Mt_conv for each k)
Mw_simple: Right hand matrix
Mb_simple: Block hand matrix
diff: objective cost</p>
<p>Note:
This code extends HALS to allow for shifting on the 3rd dimension of the tensor. Suffix '_simple' is added to
the non-convolutional components. Convolutional components are named the usual way.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NTFSolve_conv(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NTFNConv, NMFPriors, myStatusBox):
    &#34;&#34;&#34;Estimate NTF matrices (HALS)
     Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         Mb0: Initial block hand matrix
         nc: NTF rank
         tolerance: Convergence threshold
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
         NMFFixUserRHE: = 1 =&gt; fixed  right hand matrix columns
         NMFFixUserBHE: = 1 =&gt; fixed  block hand matrix columns
         NMFSparseLevel : sparsity level (as defined by Hoyer); +/- = make RHE/LHe sparse
         NTFUnimodal: Apply Unimodal constraint on factoring vectors
         NTFSmooth: Apply Smooth constraint on factoring vectors
         NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
         NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
         NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
         NBlocks: Number of NTF blocks
         NTFNConv: Half-Size of the convolution window on 3rd-dimension of the tensor
         NMFPriors: Elements in Mw that should be updated (others remain 0)

     Output:
         Mt : if NTFNConv &gt; 0 only otherwise empty. Contains sub-components for each phase in convolution window
         Mt_simple: Left hand matrix (sum of columns Mt_conv for each k)
         Mw_simple: Right hand matrix
         Mb_simple: Block hand matrix
         diff: objective cost
    
     Note: 
         This code extends HALS to allow for shifting on the 3rd dimension of the tensor. Suffix &#39;_simple&#39; is added to 
         the non-convolutional components. Convolutional components are named the usual way.

     &#34;&#34;&#34;

    cancel_pressed = 0

    n, p0 = M.shape
    n_Mmis = Mmis.shape[0]
    nc = int(nc)
    NBlocks = int(NBlocks)
    NTFNConv = int(NTFNConv)
    p = int(p0 / NBlocks)
    nxp = int(n * p)
    nxp0 = int(n * p0)
    Mt_simple = np.copy(Mt0)
    Mw_simple = np.copy(Mw0)
    Mb_simple = np.copy(Mb0)
    #     StepIter = math.ceil(MaxIterations/10)
    StepIter = 1
    pbar_step = 100 * StepIter / MaxIterations

    IDBlockp = np.arange(0, (NBlocks - 1) * p + 1, p)
    A = np.zeros(n)
    B = np.zeros(p)
    C = np.zeros(NBlocks)
    MtMw = np.zeros(nxp)
    NTFNConv2 = 2*NTFNConv + 1
    
    #Initialize Mt, Mw, Mb
    Mt = np.repeat(Mt_simple, NTFNConv2, axis=1) / NTFNConv2
    Mw = np.repeat(Mw_simple, NTFNConv2, axis=1)
    Mb = np.repeat(Mb_simple, NTFNConv2, axis=1)

    for k3 in range(0, nc):
        n_shift = -NTFNConv - 1
        for k2 in range(0, NTFNConv2):
            n_shift += 1
            k = k3*NTFNConv2+k2
            Mb[:,k] = shift(Mb_simple[:, k3], n_shift)

    # Initialize Residual tensor
    Mfit = np.zeros((n, p0))
    for k3 in range(0, nc):
        for k2 in range(0, NTFNConv2):
            k = k3*NTFNConv2+k2
            for iBlock in range(0, NBlocks):
                Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock,k] * \
                    np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))

    denomt = np.zeros(n)
    denomw = np.zeros(p)
    denomBlock = np.zeros((NBlocks, nc))
    Mt2 = np.zeros(n)
    Mw2 = np.zeros(p)
    denomCutoff = .1

    if n_Mmis &gt; 0:
        Mres = (M - Mfit) * Mmis
    else:
        Mres = M - Mfit

    myStatusBox.init_bar(delay=1)

    # Loop
    cont = 1
    iIter = 0
    diff0 = 1.e+99
    Mpart = np.zeros((n, p0)) 
    alpha = NMFSparseLevel
    alpha_blocks = 0
    PercentZeros = 0
    iterSparse = 0

    while (cont &gt; 0) &amp; (iIter &lt; MaxIterations):
        for k3 in range(0, nc):
            for k2 in range(0, NTFNConv2):
                k = k3*NTFNConv2+k2
                NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                    NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
                    NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                    denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                    denomBlock, NTFBlockComponents, C, Mfit, NMFPriors = \
                NTFUpdate(NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                    NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha, \
                    NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                    denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                    denomBlock, NTFBlockComponents, C, Mfit, NMFPriors)
            
            #Update Mt_simple, Mw_simple &amp; Mb_simple
            k = k3*NTFNConv2+NTFNConv
            Mt_simple[:, k3] = Mt[:, k]
            Mw_simple[:, k3] = Mw[:, k]
            Mb_simple[:, k3] = Mb[:, k]

            # Update Mw &amp; Mb
            Mw[:,:] = np.repeat(Mw_simple, NTFNConv2, axis=1)
            n_shift = -NTFNConv - 1
            for k2 in range(0, NTFNConv2):
                n_shift += 1
                k = k3*NTFNConv2+k2
                Mb[:,k] = shift(Mb_simple[:, k3], n_shift)
            
        if iIter % StepIter == 0:
            # Check convergence
            diff = np.linalg.norm(Mres) ** 2 / nxp0
            if (diff0 - diff) / diff0 &lt; tolerance:
                cont = 0
            else:
                diff0 = diff

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)

            if NMFSparseLevel != 0:
                Status = Status + &#39;; Achieved sparsity: &#39; + str(round(PercentZeros, 2)) + &#39;; alpha: &#39; + str(
                    round(alpha, 2))
                if LogIter == 1:
                    myStatusBox.myPrint(Status)

            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [Mt, Mt_simple, Mw_simple, Mb_simple, cancel_pressed]

            if LogIter == 1:
                myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))

        iIter += 1

        if (cont == 0) | (iIter == MaxIterations):
            if NMFSparseLevel &gt; 0:
                SparseTest = np.zeros((p, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mw[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / p, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.01 * NMFSparseLevel / PercentZeros, 1.01)
                    if alpha &lt; .99:
                        iIter = 1
                        cont = 1

            elif NMFSparseLevel &lt; 0:
                SparseTest = np.zeros((n, 1))
                for k in range(0, nc):
                    SparseTest[np.where(Mt[:, k] &gt; 0)] = 1

                PercentZeros0 = PercentZeros
                n_SparseTest = np.where(SparseTest == 0)[0].size
                PercentZeros = max(n_SparseTest / n, .01)
                if PercentZeros == PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.01 * abs(NMFSparseLevel) / PercentZeros, 1.01)
                    if abs(alpha) &lt; .99:
                        iIter = 1
                        cont = 1

    if (n_Mmis &gt; 0) &amp; (NMFFixUserBHE == 0):
        Mb *= denomBlock

    return [Mt, Mt_simple, Mw_simple, Mb_simple, diff, cancel_pressed]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NTFSolve_simple"><code class="name flex">
<span>def <span class="ident">NTFSolve_simple</span></span>(<span>M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE, NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NMFPriors, myStatusBox)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate NTF matrices (HALS)</p>
<h2 id="input">Input</h2>
<p>M: Input matrix
Mmis: Define missing values (0 = missing cell, 1 = real cell)
Mt0: Initial left hand matrix
Mw0: Initial right hand matrix
Mb0: Initial block hand matrix
nc: NTF rank
tolerance: Convergence threshold
LogIter: Log results through iterations
Status0: Initial displayed status to be updated during iterations
MaxIterations: Max iterations
NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
NMFFixUserRHE: = 1 =&gt; fixed
right hand matrix columns
NMFFixUserBHE: = 1 =&gt; fixed
block hand matrix columns
NMFSparseLevel : sparsity level (as defined by Hoyer); +/- = make RHE/LHe sparse
NTFUnimodal: Apply Unimodal constraint on factoring vectors
NTFSmooth: Apply Smooth constraint on factoring vectors
NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
NBlocks: Number of NTF blocks
NMFPriors: Elements in Mw that should be updated (others remain 0)</p>
<h2 id="output">Output</h2>
<p>Mt: Left hand matrix
Mw: Right hand matrix
Mb: Block hand matrix
diff: objective cost</p>
<h2 id="reference">Reference</h2>
<p>A. Cichocki, P.H.A.N. Anh-Huym, Fast local algorithms for large scale nonnegative matrix and tensor factorizations,
IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 92 (3) (2009) 708–721.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NTFSolve_simple(M, Mmis, Mt0, Mw0, Mb0, nc, tolerance, LogIter, Status0, MaxIterations, NMFFixUserLHE, NMFFixUserRHE, NMFFixUserBHE,
             NMFSparseLevel, NTFUnimodal, NTFSmooth, NTFLeftComponents, NTFRightComponents, NTFBlockComponents, NBlocks, NMFPriors, myStatusBox):
    &#34;&#34;&#34;
    Estimate NTF matrices (HALS)
    Input:
         M: Input matrix
         Mmis: Define missing values (0 = missing cell, 1 = real cell)
         Mt0: Initial left hand matrix
         Mw0: Initial right hand matrix
         Mb0: Initial block hand matrix
         nc: NTF rank
         tolerance: Convergence threshold
         LogIter: Log results through iterations
         Status0: Initial displayed status to be updated during iterations
         MaxIterations: Max iterations
         NMFFixUserLHE: = 1 =&gt; fixed left hand matrix columns
         NMFFixUserRHE: = 1 =&gt; fixed  right hand matrix columns
         NMFFixUserBHE: = 1 =&gt; fixed  block hand matrix columns
         NMFSparseLevel : sparsity level (as defined by Hoyer); +/- = make RHE/LHe sparse
         NTFUnimodal: Apply Unimodal constraint on factoring vectors
         NTFSmooth: Apply Smooth constraint on factoring vectors
         NTFLeftComponents: Apply Unimodal/Smooth constraint on left hand matrix
         NTFRightComponents: Apply Unimodal/Smooth constraint on right hand matrix
         NTFBlockComponents: Apply Unimodal/Smooth constraint on block hand matrix
         NBlocks: Number of NTF blocks
         NMFPriors: Elements in Mw that should be updated (others remain 0)

    Output:
         Mt: Left hand matrix
         Mw: Right hand matrix
         Mb: Block hand matrix
         diff: objective cost
    
    Reference
    ---------

    A. Cichocki, P.H.A.N. Anh-Huym, Fast local algorithms for large scale nonnegative matrix and tensor factorizations,
        IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 92 (3) (2009) 708–721.

    &#34;&#34;&#34;

    cancel_pressed = 0


    n, p0 = M.shape
    n_Mmis = Mmis.shape[0]
    nc = int(nc)
    NBlocks = int(NBlocks)
    p = int(p0 / NBlocks)
    nxp = int(n * p)
    nxp0 = int(n * p0)
    Mt = np.copy(Mt0)
    Mw = np.copy(Mw0)
    Mb = np.copy(Mb0)
    #     StepIter = math.ceil(MaxIterations/10)
    StepIter = 1
    pbar_step = 100 * StepIter / MaxIterations
 
    IDBlockp = np.arange(0, (NBlocks - 1) * p + 1, p)
    A = np.zeros(n)
    B = np.zeros(p)
    C = np.zeros(NBlocks)

    # Compute Residual tensor
    Mfit = np.zeros((n, p0))
    for k in range(0, nc):
        if NBlocks &gt; 1:
            for iBlock in range(0, NBlocks):
                Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * np.reshape(Mt[:, k], (n, 1)) @ np.reshape(
                    Mw[:, k], (1, p))
        else:
            Mfit[:, IDBlockp[0]:IDBlockp[0] + p] += np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))

    denomt = np.zeros(n)
    denomw = np.zeros(p)
    denomBlock = np.zeros((NBlocks, nc))
    Mt2 = np.zeros(n)
    Mw2 = np.zeros(p)
    MtMw = np.zeros(nxp)
    denomCutoff = .1

    if n_Mmis &gt; 0:
        Mres = (M - Mfit) * Mmis
    else:
        Mres = M - Mfit

    myStatusBox.init_bar(delay=1)

    # Loop
    cont = 1
    iIter = 0
    diff0 = 1.e+99
    Mpart = np.zeros((n, p0))
    # alpha = NMFSparseLevel
    alpha = NMFSparseLevel * .8
    PercentZeros = 0
    iterSparse = 0
    
    while (cont &gt; 0) &amp; (iIter &lt; MaxIterations):
        for k in range(0, nc):
            NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
                NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                denomBlock, NTFBlockComponents, C, Mfit, NMFPriors = \
            NTFUpdate(NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
                NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
                NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
                denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
                denomBlock, NTFBlockComponents, C, Mfit, NMFPriors)
                       
        if iIter % StepIter == 0:
            # Check convergence
            diff = np.linalg.norm(Mres) ** 2 / nxp0
            if (diff0 - diff) / diff0 &lt; tolerance:
                cont = 0                    
            else:
                if diff &gt; diff0:
                    myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR does not improve&#34;)

                diff0 = diff

            Status = Status0 + &#39;Iteration: %s&#39; % int(iIter)

            if NMFSparseLevel != 0:
                Status = Status + &#39;; Achieved sparsity: &#39; + str(round(PercentZeros, 2)) + &#39;; alpha: &#39; + str(
                    round(alpha, 2))
                if LogIter == 1:
                    myStatusBox.myPrint(Status)

            myStatusBox.update_status(delay=1, status=Status)
            myStatusBox.update_bar(delay=1, step=pbar_step)
            if myStatusBox.cancel_pressed:
                cancel_pressed = 1
                return [np.array([]), Mt, Mw, Mb, Mres, cancel_pressed]

            if LogIter == 1:
                myStatusBox.myPrint(Status0 + &#34; Iter: &#34; + str(iIter) + &#34; MSR: &#34; + str(diff))

        iIter += 1

        if (cont == 0) | (iIter == MaxIterations):
            # if NMFSparseLevel &gt; 0:
            #     SparseTest = np.zeros((p, 1))
            #     for k in range(0, nc):
            #         SparseTest[np.where(Mw[:, k] &gt; 0)] = 1

            #     PercentZeros0 = PercentZeros
            #     n_SparseTest = np.where(SparseTest == 0)[0].size
            #     PercentZeros = max(n_SparseTest / p, .01)
            #     if PercentZeros == PercentZeros0:
            #         iterSparse += 1
            #     else:
            #         iterSparse = 0

            #     if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
            #         alpha *= min(1.01 * NMFSparseLevel / PercentZeros, 1.01)
            #         if alpha &lt; .99:
            #             iIter = 1
            #             cont = 1

            # elif NMFSparseLevel &lt; 0:
            #     SparseTest = np.zeros((n, 1))
            #     for k in range(0, nc):
            #         SparseTest[np.where(Mt[:, k] &gt; 0)] = 1

            #     PercentZeros0 = PercentZeros
            #     n_SparseTest = np.where(SparseTest == 0)[0].size
            #     PercentZeros = max(n_SparseTest / n, .01)
            #     if PercentZeros == PercentZeros0:
            #         iterSparse += 1
            #     else:
            #         iterSparse = 0

            #     if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
            #         alpha *= min(1.01 * abs(NMFSparseLevel) / PercentZeros, 1.01)
            #         if abs(alpha) &lt; .99:
            #             iIter = 1
            #             cont = 1
  
            if NMFSparseLevel &gt; 0:
                SparseTest = np.zeros((nc, 1))
                PercentZeros0 = PercentZeros
                for k in range(0, nc):
                    SparseTest[k] = np.where(Mw[:, k] == 0)[0].size
                
                PercentZeros = np.mean(SparseTest) / p
                if PercentZeros &lt; PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * NMFSparseLevel) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.05 * NMFSparseLevel / PercentZeros, 1.1)
                    if alpha &lt; 1:
                        iIter = 1
                        cont = 1

            elif NMFSparseLevel &lt; 0:
                SparseTest = np.zeros((nc, 1))
                PercentZeros0 = PercentZeros
                for k in range(0, nc):
                    SparseTest[k] = np.where(Mw[:, k] == 0)[0].size
                
                PercentZeros = np.mean(SparseTest) / n
                if PercentZeros &lt; PercentZeros0:
                    iterSparse += 1
                else:
                    iterSparse = 0

                if (PercentZeros &lt; 0.99 * abs(NMFSparseLevel)) &amp; (iterSparse &lt; 50):
                    alpha *= min(1.05 * abs(NMFSparseLevel) / PercentZeros, 1.1)
                    if abs(alpha) &lt; 1:
                        iIter = 1
                        cont = 1

    if (n_Mmis &gt; 0) &amp; (NMFFixUserBHE == 0):
        Mb *= denomBlock

    return [np.array([]), Mt, Mw, Mb, diff, cancel_pressed]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NTFStack"><code class="name flex">
<span>def <span class="ident">NTFStack</span></span>(<span>M, Mmis, NBlocks)</span>
</code></dt>
<dd>
<div class="desc"><p>Unfold tensor M
for future use with NMF</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NTFStack(M, Mmis, NBlocks):
    &#34;&#34;&#34;Unfold tensor M
        for future use with NMF
    &#34;&#34;&#34;
    n, p = M.shape
    Mmis = Mmis.astype(np.int)
    n_Mmis = Mmis.shape[0]
    NBlocks = int(NBlocks)

    Mstacked = np.zeros((int(n * p / NBlocks), NBlocks))
    if n_Mmis &gt; 0:
        Mmis_stacked = np.zeros((int(n * p / NBlocks), NBlocks))
    else:
        Mmis_stacked = np.array([])

    for iBlock in range(0, NBlocks):
        for j in range(0, int(p / NBlocks)):
            i1 = j * n
            i2 = i1 + n
            Mstacked[i1:i2, iBlock] = M[:, int(iBlock * p / NBlocks + j)]
            if n_Mmis &gt; 0:
                Mmis_stacked[i1:i2, iBlock] = Mmis[:, int(iBlock * p / NBlocks + j)]

    return [Mstacked, Mmis_stacked]</code></pre>
</details>
</dd>
<dt id="nmtf.modules.nmtf_core.NTFUpdate"><code class="name flex">
<span>def <span class="ident">NTFUpdate</span></span>(<span>NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha, NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, denomBlock, NTFBlockComponents, C, Mfit, NMFPriors)</span>
</code></dt>
<dd>
<div class="desc"><p>Core updating code called by NTFSolve_simple &amp; NTF Solve_conv</p>
<h2 id="input">Input</h2>
<p>All variables in the calling function used in the function </p>
<h2 id="output">Output</h2>
<p>Same as Input</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NTFUpdate(NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
        NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha, \
        NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
        denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
        denomBlock, NTFBlockComponents, C, Mfit, NMFPriors):
    &#34;&#34;&#34;Core updating code called by NTFSolve_simple &amp; NTF Solve_conv
    Input:
        All variables in the calling function used in the function 
    Output:
        Same as Input
    &#34;&#34;&#34;

    try:
        n_NMFPriors, nc = NMFPriors.shape
    except:
        n_NMFPriors = 0

    # Compute kth-part
    if NBlocks &gt; 1:
        for iBlock in range(0, NBlocks):
            Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] = Mb[iBlock, k] * \
                np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))
    else:
        Mpart[:, IDBlockp[0]:IDBlockp[0] + p] = np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))

    if n_Mmis &gt; 0:
        Mpart *= Mmis

    Mpart += Mres 

    if NMFFixUserBHE &gt; 0:
        NormBHE = True
        if NMFFixUserRHE == 0:
            NormLHE = True
            NormRHE = False
        else:
            NormLHE = False
            NormRHE = True
    else:
            NormBHE = False
            NormLHE = True
            NormRHE = True

    if (NMFFixUserLHE &gt; 0) &amp; NormLHE:
        norm = np.linalg.norm(Mt[:, k])
        if norm &gt; 0:
            Mt[:, k] /= norm

    if (NMFFixUserRHE &gt; 0) &amp; NormRHE:
        norm = np.linalg.norm(Mw[:, k])
        if norm &gt; 0:
            Mw[:, k] /= norm
        
    if (NMFFixUserBHE &gt; 0) &amp; NormBHE &amp; (NBlocks &gt; 1):
        norm = np.linalg.norm(Mb[:, k])
        if norm &gt; 0:
            Mb[:, k] /= norm

    if NMFFixUserLHE == 0:
        # Update Mt
        Mt[:, k] = 0
        if NBlocks &gt; 1:
            for iBlock in range(0, NBlocks):
                Mt[:, k] += Mb[iBlock, k] * Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw[:, k]
        else:
            Mt[:, k] += Mpart[:, IDBlockp[0]:IDBlockp[0] + p] @ Mw[:, k]

        if n_Mmis &gt; 0:
            denomt[:] = 0
            Mw2[:] = Mw[:, k] ** 2
            if NBlocks &gt; 1:
                for iBlock in range(0, NBlocks):
                    # Broadcast missing cells into Mw to calculate Mw.T * Mw
                    denomt += Mb[iBlock, k]**2 * Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] @ Mw2
            else:
                denomt += Mmis[:, IDBlockp[0]:IDBlockp[0] + p] @ Mw2

            denomt /= np.max(denomt)
            denomt[denomt &lt; denomCutoff] = denomCutoff
            Mt[:, k] /= denomt               

        Mt[Mt[:, k] &lt; 0, k] = 0
        if alpha &lt; 0:
            Mt[:, k] = sparse_opt(Mt[:, k], -alpha, False)
         
        if (NTFUnimodal &gt; 0) &amp; (NTFLeftComponents &gt; 0):
            #                 Enforce unimodal distribution
            tmax = np.argmax(Mt[:, k])
            for i in range(tmax + 1, n):
                Mt[i, k] = min(Mt[i - 1, k], Mt[i, k])

            for i in range(tmax - 1, -1, -1):
                Mt[i, k] = min(Mt[i + 1, k], Mt[i, k])

        if (NTFSmooth &gt; 0) &amp; (NTFLeftComponents &gt; 0):
            #             Smooth distribution
            A[0] = .75 * Mt[0, k] + .25 * Mt[1, k]
            A[n - 1] = .25 * Mt[n - 2, k] + .75 * Mt[n - 1, k]
            for i in range(1, n - 1):
                A[i] = .25 * Mt[i - 1, k] + .5 * Mt[i, k] + .25 * Mt[i + 1, k]

            Mt[:, k] = A

        if NormLHE:
            norm = np.linalg.norm(Mt[:, k])
            if norm &gt; 0:
                Mt[:, k] /= norm

    if NMFFixUserRHE == 0:
        # Update Mw
        
        Mw[:, k] = 0
        if NBlocks &gt; 1:
            for iBlock in range(0, NBlocks):
                Mw[:, k] += Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T @ Mt[:, k] * Mb[iBlock, k]
        else:
            Mw[:, k] += Mpart[:, IDBlockp[0]:IDBlockp[0] + p].T @ Mt[:, k]

        if n_Mmis &gt; 0:
            denomw[:] = 0
            Mt2[:] = Mt[:, k] ** 2
            if NBlocks &gt; 1:
                for iBlock in range(0, NBlocks):
                    # Broadcast missing cells into Mw to calculate Mt.T * Mt
                    denomw += Mb[iBlock, k] ** 2 * Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p].T @ Mt2
            else:
                denomw += Mmis[:, IDBlockp[0]:IDBlockp[0] + p].T @ Mt2

            denomw /= np.max(denomw)
            denomw[denomw &lt; denomCutoff] = denomCutoff
            Mw[:, k] /= denomw

        Mw[Mw[:, k] &lt; 0, k] = 0

        if alpha &gt; 0:
            Mw[:, k] = sparse_opt(Mw[:, k], alpha, False)

        if (NTFUnimodal &gt; 0) &amp; (NTFRightComponents &gt; 0):
            #Enforce unimodal distribution
            wmax = np.argmax(Mw[:, k])
            for j in range(wmax + 1, p):
                Mw[j, k] = min(Mw[j - 1, k], Mw[j, k])

            for j in range(wmax - 1, -1, -1):
                Mw[j, k] = min(Mw[j + 1, k], Mw[j, k])

        if (NTFSmooth &gt; 0) &amp; (NTFRightComponents &gt; 0):
            #             Smooth distribution
            B[0] = .75 * Mw[0, k] + .25 * Mw[1, k]
            B[p - 1] = .25 * Mw[p - 2, k] + .75 * Mw[p - 1, k]
            for j in range(1, p - 1):
                B[j] = .25 * Mw[j - 1, k] + .5 * Mw[j, k] + .25 * Mw[j + 1, k]

            Mw[:, k] = B

        if n_NMFPriors &gt; 0:
            Mw[:, k] = Mw[:, k] * NMFPriors[:, k]

        if NormRHE:
            norm = np.linalg.norm(Mw[:, k])
            if norm &gt; 0:
                Mw[:, k] /= norm

    if NMFFixUserBHE == 0:
        # Update Mb
        Mb[:, k] = 0
        MtMw[:] = np.reshape((np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))), nxp)

        for iBlock in range(0, NBlocks):
            Mb[iBlock, k] = np.reshape(Mpart[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], nxp).T @ MtMw

        if n_Mmis &gt; 0:                          
            MtMw[:] = MtMw[:] ** 2
            for iBlock in range(0, NBlocks):
                # Broadcast missing cells into Mb to calculate Mb.T * Mb
                denomBlock[iBlock, k] = np.reshape(Mmis[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p], (1, nxp)) @ MtMw

            maxdenomBlock = np.max(denomBlock[:, k])
            denomBlock[denomBlock[:, k] &lt; denomCutoff * maxdenomBlock] = denomCutoff * maxdenomBlock
            Mb[:, k] /= denomBlock[:, k]

        Mb[Mb[:, k] &lt; 0, k] = 0

        if (NTFUnimodal &gt; 0) &amp; (NTFBlockComponents &gt; 0):
            #                 Enforce unimodal distribution
            bmax = np.argmax(Mb[:, k])
            for iBlock in range(bmax + 1, NBlocks):
                Mb[iBlock, k] = min(Mb[iBlock - 1, k], Mb[iBlock, k])

            for iBlock in range(bmax - 1, -1, -1):
                Mb[iBlock, k] = min(Mb[iBlock + 1, k], Mb[iBlock, k])

        if (NTFSmooth &gt; 0) &amp; (NTFBlockComponents &gt; 0):
            #             Smooth distribution
            C[0] = .75 * Mb[0, k] + .25 * Mb[1, k]
            C[NBlocks - 1] = .25 * Mb[NBlocks - 2, k] + .75 * Mb[NBlocks - 1, k]
            for iBlock in range(1, NBlocks - 1):
                C[iBlock] = .25 * Mb[iBlock - 1, k] + .5 * Mb[iBlock, k] + .25 * Mb[iBlock + 1, k]

            Mb[:, k] = C
        
        if NormBHE:
            norm = np.linalg.norm(Mb[:, k])
            if norm &gt; 0:
                Mb[:, k] /= norm

    # Update residual tensor
    Mfit[:,:] = 0
    if NBlocks &gt; 1:
        for iBlock in range(0, NBlocks):
            Mfit[:, IDBlockp[iBlock]:IDBlockp[iBlock] + p] += Mb[iBlock, k] * \
                np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))
    else:
        Mfit[:, IDBlockp[0]:IDBlockp[0] + p] += np.reshape(Mt[:, k], (n, 1)) @ np.reshape(Mw[:, k], (1, p))


    if n_Mmis &gt; 0:
        Mres[:,:] = (Mpart - Mfit) * Mmis
    else:
        Mres[:,:] = Mpart - Mfit

    return NBlocks, Mpart, IDBlockp, p, Mb, k, Mt, n, Mw, n_Mmis, Mmis, Mres, \
            NMFFixUserLHE, denomt, Mw2, denomCutoff, alpha ,\
            NTFUnimodal, NTFLeftComponents, NTFSmooth, A, NMFFixUserRHE, \
            denomw, Mt2, NTFRightComponents, B, NMFFixUserBHE, MtMw, nxp, \
            denomBlock, NTFBlockComponents, C, Mfit, NMFPriors</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nmtf.modules" href="index.html">nmtf.modules</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="nmtf.modules.nmtf_core.NMFApplyKernel" href="#nmtf.modules.nmtf_core.NMFApplyKernel">NMFApplyKernel</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NMFProjGrad" href="#nmtf.modules.nmtf_core.NMFProjGrad">NMFProjGrad</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NMFProjGradKernel" href="#nmtf.modules.nmtf_core.NMFProjGradKernel">NMFProjGradKernel</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NMFReweigh" href="#nmtf.modules.nmtf_core.NMFReweigh">NMFReweigh</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NMFSolve" href="#nmtf.modules.nmtf_core.NMFSolve">NMFSolve</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NTFSolve" href="#nmtf.modules.nmtf_core.NTFSolve">NTFSolve</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NTFSolveFast" href="#nmtf.modules.nmtf_core.NTFSolveFast">NTFSolveFast</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NTFSolve_conv" href="#nmtf.modules.nmtf_core.NTFSolve_conv">NTFSolve_conv</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NTFSolve_simple" href="#nmtf.modules.nmtf_core.NTFSolve_simple">NTFSolve_simple</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NTFStack" href="#nmtf.modules.nmtf_core.NTFStack">NTFStack</a></code></li>
<li><code><a title="nmtf.modules.nmtf_core.NTFUpdate" href="#nmtf.modules.nmtf_core.NTFUpdate">NTFUpdate</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>